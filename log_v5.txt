Initial training for the first task.
100%|██████████| 79/79 [01:17<00:00,  1.01it/s]
Task 0, Epoch 1/40 => Loss 2.274, Train_accy 22.78, Test_accy 42.20
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 2/40 => Loss 2.173, Train_accy 43.16, Test_accy 48.60
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 3/40 => Loss 2.028, Train_accy 44.16, Test_accy 50.90
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 4/40 => Loss 1.878, Train_accy 47.94, Test_accy 53.00
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 5/40 => Loss 1.762, Train_accy 48.34, Test_accy 58.30
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 6/40 => Loss 1.658, Train_accy 50.76, Test_accy 59.70
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 7/40 => Loss 1.569, Train_accy 53.46, Test_accy 62.10
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 8/40 => Loss 1.521, Train_accy 53.08, Test_accy 62.90
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 9/40 => Loss 1.464, Train_accy 54.76, Test_accy 65.00
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 10/40 => Loss 1.416, Train_accy 54.90, Test_accy 64.50
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 11/40 => Loss 1.369, Train_accy 56.90, Test_accy 66.60
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 12/40 => Loss 1.327, Train_accy 58.76, Test_accy 66.50
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 13/40 => Loss 1.288, Train_accy 60.24, Test_accy 67.40
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 14/40 => Loss 1.251, Train_accy 60.58, Test_accy 71.40
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 15/40 => Loss 1.227, Train_accy 61.42, Test_accy 69.70
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 16/40 => Loss 1.197, Train_accy 61.96, Test_accy 72.30
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 17/40 => Loss 1.179, Train_accy 63.04, Test_accy 72.70
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 18/40 => Loss 1.135, Train_accy 63.78, Test_accy 74.00
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 19/40 => Loss 1.105, Train_accy 65.56, Test_accy 75.10
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 20/40 => Loss 1.099, Train_accy 65.02, Test_accy 76.20
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 21/40 => Loss 1.076, Train_accy 65.98, Test_accy 76.10
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 22/40 => Loss 1.040, Train_accy 67.88, Test_accy 77.50
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 23/40 => Loss 1.021, Train_accy 68.24, Test_accy 78.20
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 24/40 => Loss 1.004, Train_accy 69.14, Test_accy 79.30
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 25/40 => Loss 0.982, Train_accy 68.94, Test_accy 79.20
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 26/40 => Loss 0.970, Train_accy 70.12, Test_accy 81.10
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 27/40 => Loss 0.935, Train_accy 70.22, Test_accy 81.40
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 28/40 => Loss 0.951, Train_accy 69.88, Test_accy 81.90
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 29/40 => Loss 0.933, Train_accy 71.22, Test_accy 82.20
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 30/40 => Loss 0.928, Train_accy 71.18, Test_accy 82.50
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 31/40 => Loss 0.920, Train_accy 72.12, Test_accy 82.40
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 32/40 => Loss 0.898, Train_accy 72.02, Test_accy 82.60
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 33/40 => Loss 0.892, Train_accy 72.06, Test_accy 82.60
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 34/40 => Loss 0.901, Train_accy 72.86, Test_accy 82.70
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 35/40 => Loss 0.892, Train_accy 72.80, Test_accy 83.10
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 36/40 => Loss 0.892, Train_accy 72.44, Test_accy 83.20
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 37/40 => Loss 0.890, Train_accy 72.30, Test_accy 83.30
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 38/40 => Loss 0.881, Train_accy 72.72, Test_accy 83.50
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 39/40 => Loss 0.882, Train_accy 73.10, Test_accy 83.40
100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 40/40 => Loss 0.875, Train_accy 73.20, Test_accy 83.50
This is for the BaseNet initialization.
After BaseNet initialization.
Clear the backbone in MultiBranchCosineIncrementalNet, since we are using self.backbones with dual branches
Constructed dual branch network.
New network structure:
MultiBranchCosineIncrementalNet_adapt_AC(
  (backbone): Identity()
  (backbones): ModuleList(
    (0): VisionTransformerBiLoRA(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): Sequential(
        (0): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (8): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (9): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (10): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (11): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (pre_logits): Identity()
      (fc_norm): Identity()
      (head): Identity()
    )
  )
  (fc): CosineLinear2()
)
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=10, bias=False)
  )
)
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.90it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 0 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.91it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 1 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.93it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 2 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.90it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 3 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.91it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 4 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.91it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 5 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.91it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 6 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.91it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 7 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.90it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 8 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.92it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 9 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Starting class alignment...
Alignment: 100%|██████████| 79/79 [00:37<00:00,  2.08batch/s]
Embedding shape:  torch.Size([5000, 5000])
Label shape torch.Size([5000])
One-hot label shape:  torch.Size([5000, 10])
Optimising ridge parameter...
selected lambda = 10000.0
gamma 10000.0
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 0 Elapsed time:  3497.2132613658905
Average Accuracy (CNN): 86.8
Learning on 10-20
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([10, 768])
New weight (Cosine FC) torch.Size([20, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=20, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([10, 5000])
New weight (AC model) torch.Size([20, 5000])
118,825,505 total parameters.
29,086,849 training parameters.
Progressive training for task 1
100%|██████████| 79/79 [01:19<00:00,  1.00s/it]
Task 1, Epoch 1/25 => Loss 2.483, Train_accy 20.54, Test_accy 10.85
100%|██████████| 79/79 [01:19<00:00,  1.00s/it]
Task 1, Epoch 2/25 => Loss 2.268, Train_accy 22.86, Test_accy 11.20
100%|██████████| 79/79 [01:19<00:00,  1.00s/it]
Task 1, Epoch 3/25 => Loss 2.215, Train_accy 24.70, Test_accy 11.90
100%|██████████| 79/79 [01:19<00:00,  1.00s/it]
Task 1, Epoch 4/25 => Loss 2.183, Train_accy 26.34, Test_accy 12.40
100%|██████████| 79/79 [01:19<00:00,  1.00s/it]
Task 1, Epoch 5/25 => Loss 2.159, Train_accy 27.06, Test_accy 13.55
100%|██████████| 79/79 [01:19<00:00,  1.00s/it]
Task 1, Epoch 6/25 => Loss 2.132, Train_accy 30.22, Test_accy 14.30
100%|██████████| 79/79 [01:19<00:00,  1.00s/it]
Task 1, Epoch 7/25 => Loss 2.112, Train_accy 30.82, Test_accy 14.75
100%|██████████| 79/79 [01:19<00:00,  1.00s/it]
Task 1, Epoch 8/25 => Loss 2.089, Train_accy 33.50, Test_accy 15.60
100%|██████████| 79/79 [01:19<00:00,  1.00s/it]
Task 1, Epoch 9/25 => Loss 2.073, Train_accy 34.10, Test_accy 15.85
100%|██████████| 79/79 [01:19<00:00,  1.00s/it]
Task 1, Epoch 10/25 => Loss 2.052, Train_accy 34.16, Test_accy 16.15
100%|██████████| 79/79 [01:19<00:00,  1.00s/it]
Task 1, Epoch 11/25 => Loss 2.042, Train_accy 34.62, Test_accy 16.40
100%|██████████| 79/79 [01:18<00:00,  1.00it/s]
Task 1, Epoch 12/25 => Loss 2.025, Train_accy 35.70, Test_accy 17.00
100%|██████████| 79/79 [01:18<00:00,  1.00it/s]
Task 1, Epoch 13/25 => Loss 2.020, Train_accy 35.24, Test_accy 17.25
100%|██████████| 79/79 [01:18<00:00,  1.00it/s]
Task 1, Epoch 14/25 => Loss 2.001, Train_accy 36.20, Test_accy 17.50
100%|██████████| 79/79 [01:18<00:00,  1.00it/s]
Task 1, Epoch 15/25 => Loss 1.994, Train_accy 35.68, Test_accy 17.75
100%|██████████| 79/79 [01:18<00:00,  1.00it/s]
Task 1, Epoch 16/25 => Loss 1.987, Train_accy 36.76, Test_accy 17.95
100%|██████████| 79/79 [01:18<00:00,  1.00it/s]
Task 1, Epoch 17/25 => Loss 1.982, Train_accy 36.84, Test_accy 18.15
100%|██████████| 79/79 [01:18<00:00,  1.00it/s]
Task 1, Epoch 18/25 => Loss 1.975, Train_accy 36.66, Test_accy 18.05
100%|██████████| 79/79 [01:18<00:00,  1.00it/s]
Task 1, Epoch 19/25 => Loss 1.976, Train_accy 36.62, Test_accy 18.25
100%|██████████| 79/79 [01:18<00:00,  1.00it/s]
Task 1, Epoch 20/25 => Loss 1.971, Train_accy 36.40, Test_accy 18.25
100%|██████████| 79/79 [01:18<00:00,  1.00it/s]
Task 1, Epoch 21/25 => Loss 1.961, Train_accy 37.40, Test_accy 18.25
100%|██████████| 79/79 [01:18<00:00,  1.00it/s]
Task 1, Epoch 22/25 => Loss 1.954, Train_accy 36.74, Test_accy 18.20
100%|██████████| 79/79 [01:18<00:00,  1.00it/s]
Task 1, Epoch 23/25 => Loss 1.961, Train_accy 37.14, Test_accy 18.20
100%|██████████| 79/79 [01:18<00:00,  1.00it/s]
Task 1, Epoch 24/25 => Loss 1.965, Train_accy 37.54, Test_accy 18.20
100%|██████████| 79/79 [01:19<00:00,  1.00s/it]
Task 1, Epoch 25/25 => Loss 1.959, Train_accy 36.64, Test_accy 18.20
Cache size: 79
Task 1, Epoch 25/25 => Loss 1.959, Train_accy 36.64, Test_accy 18.20
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.83it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 10 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.84it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 11 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.83it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 12 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.84it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 13 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.84it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 14 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.84it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 15 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.83it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 16 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.82it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 17 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.85it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 18 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.83it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 19 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  0%|          | 0/50 [00:00<?, ?it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.43it/s]
best_loss: 1.0710174589157104
Computing class relations...
Old means shape: (10, 768)
New means shape: (10, 768)
Class relations: [11 12 11 14 11 13 11 15 11 11]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 10000
Feature dataset dimension: 768
Label dataset size: 10000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 20])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 157/157 [00:05<00:00, 28.35batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 1 Elapsed time:  2500.879010438919
Average Accuracy (CNN): 59.675
Learning on 20-30
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([20, 768])
New weight (Cosine FC) torch.Size([30, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=30, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([20, 5000])
New weight (AC model) torch.Size([30, 5000])
118,883,185 total parameters.
29,094,529 training parameters.
Progressive training for task 2
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 1/25 => Loss 2.319, Train_accy 18.22, Test_accy 6.93
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 2/25 => Loss 2.281, Train_accy 18.92, Test_accy 7.03
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 3/25 => Loss 2.275, Train_accy 18.78, Test_accy 7.20
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 4/25 => Loss 2.265, Train_accy 19.48, Test_accy 7.23
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 5/25 => Loss 2.259, Train_accy 18.54, Test_accy 7.20
100%|██████████| 79/79 [01:21<00:00,  1.03s/it]
Task 2, Epoch 6/25 => Loss 2.252, Train_accy 19.66, Test_accy 7.50
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 7/25 => Loss 2.248, Train_accy 19.80, Test_accy 7.67
100%|██████████| 79/79 [01:20<00:00,  1.03s/it]
Task 2, Epoch 8/25 => Loss 2.242, Train_accy 20.02, Test_accy 7.83
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 9/25 => Loss 2.236, Train_accy 19.86, Test_accy 7.93
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 10/25 => Loss 2.234, Train_accy 20.14, Test_accy 7.87
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 11/25 => Loss 2.230, Train_accy 20.76, Test_accy 7.87
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 12/25 => Loss 2.225, Train_accy 20.84, Test_accy 7.93
100%|██████████| 79/79 [01:21<00:00,  1.03s/it]
Task 2, Epoch 13/25 => Loss 2.221, Train_accy 21.06, Test_accy 8.03
100%|██████████| 79/79 [01:21<00:00,  1.03s/it]
Task 2, Epoch 14/25 => Loss 2.219, Train_accy 21.06, Test_accy 8.17
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 15/25 => Loss 2.218, Train_accy 20.94, Test_accy 8.27
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 16/25 => Loss 2.216, Train_accy 20.96, Test_accy 8.30
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 17/25 => Loss 2.212, Train_accy 22.10, Test_accy 8.33
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 18/25 => Loss 2.209, Train_accy 22.26, Test_accy 8.30
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 19/25 => Loss 2.209, Train_accy 21.58, Test_accy 8.37
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 20/25 => Loss 2.209, Train_accy 21.32, Test_accy 8.33
100%|██████████| 79/79 [01:21<00:00,  1.03s/it]
Task 2, Epoch 21/25 => Loss 2.207, Train_accy 21.90, Test_accy 8.37
100%|██████████| 79/79 [01:21<00:00,  1.03s/it]
Task 2, Epoch 22/25 => Loss 2.210, Train_accy 21.76, Test_accy 8.37
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 23/25 => Loss 2.210, Train_accy 21.68, Test_accy 8.40
100%|██████████| 79/79 [01:21<00:00,  1.03s/it]
Task 2, Epoch 24/25 => Loss 2.206, Train_accy 21.68, Test_accy 8.40
100%|██████████| 79/79 [01:21<00:00,  1.03s/it]
Task 2, Epoch 25/25 => Loss 2.209, Train_accy 21.50, Test_accy 8.40
Cache size: 79
Task 2, Epoch 25/25 => Loss 2.209, Train_accy 21.50, Test_accy 8.40
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.75it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 20 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.78it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 21 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.77it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 22 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.78it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 23 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.77it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 24 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.78it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 25 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.77it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 26 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.77it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 27 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.78it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 28 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.77it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 29 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
 2%|▏         | 1/50 [00:00<00:09,  5.19it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.44it/s]
best_loss: 0.2928900835514069
Computing class relations...
Old means shape: (20, 768)
New means shape: (10, 768)
Class relations: [24 28 28 28 26 28 21 22 22 24 25 25 28 25 25 25 25 25 25 25]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 15000
Feature dataset dimension: 768
Label dataset size: 15000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 30])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 235/235 [00:08<00:00, 27.37batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 2 Elapsed time:  2777.328757047653
Average Accuracy (CNN): 44.69333333333333
Learning on 30-40
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([30, 768])
New weight (Cosine FC) torch.Size([40, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=40, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([30, 5000])
New weight (AC model) torch.Size([40, 5000])
118,940,865 total parameters.
29,102,209 training parameters.
Progressive training for task 3
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 1/25 => Loss 3.452, Train_accy 31.12, Test_accy 8.62
100%|██████████| 79/79 [01:24<00:00,  1.07s/it]
Task 3, Epoch 2/25 => Loss 3.125, Train_accy 32.00, Test_accy 8.72
100%|██████████| 79/79 [01:24<00:00,  1.07s/it]
Task 3, Epoch 3/25 => Loss 2.892, Train_accy 32.14, Test_accy 8.52
100%|██████████| 79/79 [01:24<00:00,  1.06s/it]
Task 3, Epoch 4/25 => Loss 2.742, Train_accy 31.92, Test_accy 8.32
100%|██████████| 79/79 [01:24<00:00,  1.07s/it]
Task 3, Epoch 5/25 => Loss 2.653, Train_accy 31.64, Test_accy 8.45
100%|██████████| 79/79 [01:24<00:00,  1.06s/it]
Task 3, Epoch 6/25 => Loss 2.582, Train_accy 30.84, Test_accy 8.15
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 7/25 => Loss 2.518, Train_accy 31.08, Test_accy 8.28
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 8/25 => Loss 2.464, Train_accy 31.48, Test_accy 8.20
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 9/25 => Loss 2.419, Train_accy 30.78, Test_accy 8.15
100%|██████████| 79/79 [01:24<00:00,  1.07s/it]
Task 3, Epoch 10/25 => Loss 2.368, Train_accy 31.18, Test_accy 8.25
100%|██████████| 79/79 [01:25<00:00,  1.08s/it]
Task 3, Epoch 11/25 => Loss 2.329, Train_accy 31.76, Test_accy 8.52
100%|██████████| 79/79 [01:24<00:00,  1.07s/it]
Task 3, Epoch 12/25 => Loss 2.294, Train_accy 32.20, Test_accy 8.55
100%|██████████| 79/79 [01:25<00:00,  1.08s/it]
Task 3, Epoch 13/25 => Loss 2.267, Train_accy 32.38, Test_accy 8.65
100%|██████████| 79/79 [01:24<00:00,  1.08s/it]
Task 3, Epoch 14/25 => Loss 2.237, Train_accy 33.78, Test_accy 8.75
100%|██████████| 79/79 [01:25<00:00,  1.08s/it]
Task 3, Epoch 15/25 => Loss 2.220, Train_accy 33.16, Test_accy 8.78
100%|██████████| 79/79 [01:24<00:00,  1.08s/it]
Task 3, Epoch 16/25 => Loss 2.204, Train_accy 33.08, Test_accy 8.75
100%|██████████| 79/79 [01:24<00:00,  1.07s/it]
Task 3, Epoch 17/25 => Loss 2.189, Train_accy 33.78, Test_accy 8.75
100%|██████████| 79/79 [01:24<00:00,  1.06s/it]
Task 3, Epoch 18/25 => Loss 2.181, Train_accy 33.78, Test_accy 8.70
100%|██████████| 79/79 [01:24<00:00,  1.07s/it]
Task 3, Epoch 19/25 => Loss 2.177, Train_accy 33.78, Test_accy 8.70
100%|██████████| 79/79 [01:24<00:00,  1.07s/it]
Task 3, Epoch 20/25 => Loss 2.171, Train_accy 33.92, Test_accy 8.80
100%|██████████| 79/79 [01:24<00:00,  1.07s/it]
Task 3, Epoch 21/25 => Loss 2.171, Train_accy 34.34, Test_accy 8.80
100%|██████████| 79/79 [01:24<00:00,  1.07s/it]
Task 3, Epoch 22/25 => Loss 2.161, Train_accy 34.26, Test_accy 8.75
100%|██████████| 79/79 [01:24<00:00,  1.06s/it]
Task 3, Epoch 23/25 => Loss 2.164, Train_accy 33.70, Test_accy 8.75
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 24/25 => Loss 2.164, Train_accy 34.10, Test_accy 8.75
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 25/25 => Loss 2.159, Train_accy 34.10, Test_accy 8.75
Cache size: 79
Task 3, Epoch 25/25 => Loss 2.159, Train_accy 34.10, Test_accy 8.75
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.68it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 30 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.69it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 31 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.69it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 32 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.69it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 33 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.70it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 34 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.69it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 35 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.68it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 36 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.68it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 37 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.69it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 38 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.68it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 39 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  2%|▏         | 1/50 [00:00<00:09,  5.04it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.31it/s]
best_loss: 0.5867622165679932
Computing class relations...
Old means shape: (30, 768)
New means shape: (10, 768)
Class relations: [39 31 31 31 31 31 39 31 31 39 30 31 31 30 30 31 31 30 30 30 31 31 31 31
 31 31 31 31 31 31]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 20000
Feature dataset dimension: 768
Label dataset size: 20000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 40])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 313/313 [00:11<00:00, 27.04batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 3 Elapsed time:  3127.418113708496
Average Accuracy (CNN): 36.69
Learning on 40-50
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([40, 768])
New weight (Cosine FC) torch.Size([50, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=50, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([40, 5000])
New weight (AC model) torch.Size([50, 5000])
118,998,545 total parameters.
29,109,889 training parameters.
Progressive training for task 4
100%|██████████| 79/79 [01:27<00:00,  1.10s/it]
Task 4, Epoch 1/25 => Loss 2.119, Train_accy 35.58, Test_accy 7.48
100%|██████████| 79/79 [01:26<00:00,  1.10s/it]
Task 4, Epoch 2/25 => Loss 2.013, Train_accy 35.84, Test_accy 7.64
100%|██████████| 79/79 [01:26<00:00,  1.10s/it]
Task 4, Epoch 3/25 => Loss 1.986, Train_accy 36.04, Test_accy 7.70
100%|██████████| 79/79 [01:26<00:00,  1.10s/it]
Task 4, Epoch 4/25 => Loss 1.967, Train_accy 36.28, Test_accy 7.68
100%|██████████| 79/79 [01:26<00:00,  1.10s/it]
Task 4, Epoch 5/25 => Loss 1.945, Train_accy 37.28, Test_accy 7.82
100%|██████████| 79/79 [01:26<00:00,  1.10s/it]
Task 4, Epoch 6/25 => Loss 1.921, Train_accy 36.92, Test_accy 7.86
100%|██████████| 79/79 [01:27<00:00,  1.11s/it]
Task 4, Epoch 7/25 => Loss 1.908, Train_accy 37.20, Test_accy 8.00
100%|██████████| 79/79 [01:26<00:00,  1.10s/it]
Task 4, Epoch 8/25 => Loss 1.896, Train_accy 37.10, Test_accy 8.06
100%|██████████| 79/79 [01:26<00:00,  1.10s/it]
Task 4, Epoch 9/25 => Loss 1.883, Train_accy 37.16, Test_accy 8.08
100%|██████████| 79/79 [01:27<00:00,  1.10s/it]
Task 4, Epoch 10/25 => Loss 1.872, Train_accy 38.58, Test_accy 8.16
100%|██████████| 79/79 [01:27<00:00,  1.11s/it]
Task 4, Epoch 11/25 => Loss 1.865, Train_accy 38.20, Test_accy 8.22
100%|██████████| 79/79 [01:27<00:00,  1.11s/it]
Task 4, Epoch 12/25 => Loss 1.852, Train_accy 38.70, Test_accy 8.28
100%|██████████| 79/79 [01:27<00:00,  1.11s/it]
Task 4, Epoch 13/25 => Loss 1.852, Train_accy 38.54, Test_accy 8.32
100%|██████████| 79/79 [01:27<00:00,  1.10s/it]
Task 4, Epoch 14/25 => Loss 1.849, Train_accy 38.48, Test_accy 8.34
100%|██████████| 79/79 [01:27<00:00,  1.11s/it]
Task 4, Epoch 15/25 => Loss 1.845, Train_accy 38.58, Test_accy 8.34
100%|██████████| 79/79 [01:27<00:00,  1.11s/it]
Task 4, Epoch 16/25 => Loss 1.833, Train_accy 38.22, Test_accy 8.42
100%|██████████| 79/79 [01:27<00:00,  1.11s/it]
Task 4, Epoch 17/25 => Loss 1.822, Train_accy 39.00, Test_accy 8.50
100%|██████████| 79/79 [01:27<00:00,  1.11s/it]
Task 4, Epoch 18/25 => Loss 1.832, Train_accy 38.50, Test_accy 8.48
100%|██████████| 79/79 [01:26<00:00,  1.10s/it]
Task 4, Epoch 19/25 => Loss 1.822, Train_accy 38.92, Test_accy 8.54
100%|██████████| 79/79 [01:26<00:00,  1.09s/it]
Task 4, Epoch 20/25 => Loss 1.818, Train_accy 38.90, Test_accy 8.58
100%|██████████| 79/79 [01:26<00:00,  1.09s/it]
Task 4, Epoch 21/25 => Loss 1.827, Train_accy 39.30, Test_accy 8.54
100%|██████████| 79/79 [01:26<00:00,  1.09s/it]
Task 4, Epoch 22/25 => Loss 1.819, Train_accy 39.34, Test_accy 8.56
100%|██████████| 79/79 [01:26<00:00,  1.09s/it]
Task 4, Epoch 23/25 => Loss 1.821, Train_accy 39.22, Test_accy 8.56
100%|██████████| 79/79 [01:26<00:00,  1.10s/it]
Task 4, Epoch 24/25 => Loss 1.816, Train_accy 39.46, Test_accy 8.60
100%|██████████| 79/79 [01:27<00:00,  1.10s/it]
Task 4, Epoch 25/25 => Loss 1.813, Train_accy 39.52, Test_accy 8.60
Cache size: 79
Task 4, Epoch 25/25 => Loss 1.813, Train_accy 39.52, Test_accy 8.60
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.59it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 40 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.62it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 41 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.62it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 42 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.61it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 43 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.62it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 44 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.62it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 45 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.61it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 46 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.61it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 47 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.61it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 48 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.62it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 49 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  2%|▏         | 1/50 [00:00<00:09,  5.22it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.47it/s]
best_loss: 0.39281390023231505
Computing class relations...
Old means shape: (40, 768)
New means shape: (10, 768)
Class relations: [44 40 40 40 40 40 44 40 40 40 40 40 40 49 40 49 40 40 40 40 40 40 40 40
 40 40 40 40 40 40 44 44 44 44 44 46 44 44 44 44]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 25000
Feature dataset dimension: 768
Label dataset size: 25000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 50])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 391/391 [00:14<00:00, 27.21batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 4 Elapsed time:  3472.049667596817
Average Accuracy (CNN): 31.22
Learning on 50-60
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([50, 768])
New weight (Cosine FC) torch.Size([60, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=60, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([50, 5000])
New weight (AC model) torch.Size([60, 5000])
119,056,225 total parameters.
29,117,569 training parameters.
Progressive training for task 5
100%|██████████| 79/79 [01:29<00:00,  1.13s/it]
Task 5, Epoch 1/25 => Loss 2.242, Train_accy 23.36, Test_accy 4.35
100%|██████████| 79/79 [01:29<00:00,  1.13s/it]
Task 5, Epoch 2/25 => Loss 2.136, Train_accy 25.52, Test_accy 4.33
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 3/25 => Loss 2.129, Train_accy 24.56, Test_accy 4.37
100%|██████████| 79/79 [01:28<00:00,  1.13s/it]
Task 5, Epoch 4/25 => Loss 2.113, Train_accy 25.56, Test_accy 4.50
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 5/25 => Loss 2.101, Train_accy 26.00, Test_accy 4.55
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 6/25 => Loss 2.090, Train_accy 26.88, Test_accy 4.48
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 7/25 => Loss 2.082, Train_accy 26.44, Test_accy 4.50
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 8/25 => Loss 2.070, Train_accy 26.82, Test_accy 4.53
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 9/25 => Loss 2.066, Train_accy 28.56, Test_accy 4.62
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 10/25 => Loss 2.065, Train_accy 27.96, Test_accy 4.73
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 11/25 => Loss 2.053, Train_accy 29.60, Test_accy 4.73
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 12/25 => Loss 2.048, Train_accy 28.04, Test_accy 4.73
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 13/25 => Loss 2.044, Train_accy 28.54, Test_accy 4.90
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 14/25 => Loss 2.042, Train_accy 29.10, Test_accy 4.92
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 15/25 => Loss 2.031, Train_accy 28.70, Test_accy 5.00
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 16/25 => Loss 2.030, Train_accy 29.06, Test_accy 5.02
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 17/25 => Loss 2.033, Train_accy 29.38, Test_accy 5.07
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 18/25 => Loss 2.027, Train_accy 29.42, Test_accy 5.08
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 19/25 => Loss 2.025, Train_accy 29.90, Test_accy 5.07
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 20/25 => Loss 2.023, Train_accy 29.24, Test_accy 5.10
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 21/25 => Loss 2.024, Train_accy 29.02, Test_accy 5.10
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 22/25 => Loss 2.022, Train_accy 29.40, Test_accy 5.12
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 23/25 => Loss 2.025, Train_accy 29.74, Test_accy 5.12
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 24/25 => Loss 2.022, Train_accy 30.02, Test_accy 5.13
100%|██████████| 79/79 [01:28<00:00,  1.13s/it]
Task 5, Epoch 25/25 => Loss 2.027, Train_accy 29.70, Test_accy 5.13
Cache size: 79
Task 5, Epoch 25/25 => Loss 2.027, Train_accy 29.70, Test_accy 5.13
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.51it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 50 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.53it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 51 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.54it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 52 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.54it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 53 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.55it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 54 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.52it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 55 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.50it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 56 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.54it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 57 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.54it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 58 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.53it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 59 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  0%|          | 0/50 [00:00<?, ?it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.42it/s]
best_loss: 0.35653481912612917
Computing class relations...
Old means shape: (50, 768)
New means shape: (10, 768)
Class relations: [50 52 52 52 57 57 50 57 51 51 57 57 57 57 57 57 57 57 57 57 56 56 56 56
 56 56 56 56 56 56 51 51 51 51 51 51 51 51 51 51 51 52 52 52 51 53 53 52
 53 52]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 30000
Feature dataset dimension: 768
Label dataset size: 30000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 60])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 469/469 [00:17<00:00, 27.13batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 5 Elapsed time:  3798.775943994522
Average Accuracy (CNN): 27.40833333333333
Learning on 60-70
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([60, 768])
New weight (Cosine FC) torch.Size([70, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=70, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([60, 5000])
New weight (AC model) torch.Size([70, 5000])
119,113,905 total parameters.
29,125,249 training parameters.
Progressive training for task 6
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 1/25 => Loss 2.219, Train_accy 27.78, Test_accy 4.16
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 2/25 => Loss 2.119, Train_accy 28.70, Test_accy 4.30
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 3/25 => Loss 2.102, Train_accy 28.98, Test_accy 4.46
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 4/25 => Loss 2.086, Train_accy 29.36, Test_accy 4.54
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 5/25 => Loss 2.078, Train_accy 29.26, Test_accy 4.59
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 6/25 => Loss 2.074, Train_accy 29.58, Test_accy 4.60
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 7/25 => Loss 2.065, Train_accy 30.42, Test_accy 4.66
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 8/25 => Loss 2.053, Train_accy 30.58, Test_accy 4.69
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 9/25 => Loss 2.043, Train_accy 30.62, Test_accy 4.79
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 10/25 => Loss 2.040, Train_accy 31.04, Test_accy 4.76
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 11/25 => Loss 2.038, Train_accy 31.56, Test_accy 4.76
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 12/25 => Loss 2.029, Train_accy 30.70, Test_accy 4.81
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 13/25 => Loss 2.036, Train_accy 30.74, Test_accy 4.81
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 14/25 => Loss 2.029, Train_accy 30.88, Test_accy 4.84
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 15/25 => Loss 2.026, Train_accy 31.60, Test_accy 4.83
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 16/25 => Loss 2.019, Train_accy 31.44, Test_accy 4.87
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 17/25 => Loss 2.015, Train_accy 31.52, Test_accy 4.81
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 18/25 => Loss 2.021, Train_accy 31.24, Test_accy 4.90
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 19/25 => Loss 2.017, Train_accy 32.16, Test_accy 4.90
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 20/25 => Loss 2.020, Train_accy 31.40, Test_accy 4.89
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 21/25 => Loss 2.019, Train_accy 31.82, Test_accy 4.91
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 22/25 => Loss 2.015, Train_accy 31.64, Test_accy 4.91
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 23/25 => Loss 2.011, Train_accy 31.90, Test_accy 4.91
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 24/25 => Loss 2.020, Train_accy 30.96, Test_accy 4.91
100%|██████████| 79/79 [01:31<00:00,  1.16s/it]
Task 6, Epoch 25/25 => Loss 2.006, Train_accy 32.04, Test_accy 4.91
Cache size: 79
Task 6, Epoch 25/25 => Loss 2.006, Train_accy 32.04, Test_accy 4.91
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.44it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 60 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.49it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 61 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.48it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 62 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.48it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 63 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.49it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 64 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.48it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 65 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.49it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 66 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.48it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 67 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.49it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 68 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.48it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 69 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  0%|          | 0/50 [00:00<?, ?it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.22it/s]
best_loss: 0.2608159074783325
Computing class relations...
Old means shape: (60, 768)
New means shape: (10, 768)
Class relations: [61 60 60 60 60 60 65 60 66 61 60 60 60 60 60 60 60 60 60 60 61 61 61 61
 61 60 60 61 61 61 61 65 65 65 65 69 65 65 65 65 66 66 66 66 66 66 66 66
 66 66 61 61 64 61 61 61 61 61 61 68]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 35000
Feature dataset dimension: 768
Label dataset size: 35000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 70])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 547/547 [00:20<00:00, 27.07batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 6 Elapsed time:  4198.199683189392
Average Accuracy (CNN): 24.222857142857144
Learning on 70-80
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([70, 768])
New weight (Cosine FC) torch.Size([80, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=80, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([70, 5000])
New weight (AC model) torch.Size([80, 5000])
119,171,585 total parameters.
29,132,929 training parameters.
Progressive training for task 7
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 1/25 => Loss 2.207, Train_accy 25.82, Test_accy 3.71
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 2/25 => Loss 2.083, Train_accy 26.98, Test_accy 3.76
100%|██████████| 79/79 [01:34<00:00,  1.20s/it]
Task 7, Epoch 3/25 => Loss 2.074, Train_accy 27.00, Test_accy 3.75
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 4/25 => Loss 2.066, Train_accy 27.12, Test_accy 3.76
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 5/25 => Loss 2.054, Train_accy 28.12, Test_accy 3.80
100%|██████████| 79/79 [01:33<00:00,  1.19s/it]
Task 7, Epoch 6/25 => Loss 2.051, Train_accy 28.50, Test_accy 3.84
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 7/25 => Loss 2.041, Train_accy 28.26, Test_accy 3.80
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 8/25 => Loss 2.036, Train_accy 28.46, Test_accy 3.80
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 9/25 => Loss 2.041, Train_accy 28.60, Test_accy 3.82
100%|██████████| 79/79 [01:33<00:00,  1.19s/it]
Task 7, Epoch 10/25 => Loss 2.035, Train_accy 28.14, Test_accy 3.81
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 11/25 => Loss 2.030, Train_accy 28.80, Test_accy 3.92
100%|██████████| 79/79 [01:33<00:00,  1.19s/it]
Task 7, Epoch 12/25 => Loss 2.021, Train_accy 29.06, Test_accy 3.92
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 13/25 => Loss 2.020, Train_accy 28.84, Test_accy 3.94
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 14/25 => Loss 2.021, Train_accy 28.90, Test_accy 3.94
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 15/25 => Loss 2.012, Train_accy 29.22, Test_accy 3.98
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 16/25 => Loss 2.013, Train_accy 29.06, Test_accy 3.99
100%|██████████| 79/79 [01:33<00:00,  1.19s/it]
Task 7, Epoch 17/25 => Loss 2.008, Train_accy 29.58, Test_accy 4.06
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 18/25 => Loss 2.004, Train_accy 29.66, Test_accy 4.06
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 19/25 => Loss 2.006, Train_accy 30.06, Test_accy 4.08
100%|██████████| 79/79 [01:33<00:00,  1.19s/it]
Task 7, Epoch 20/25 => Loss 2.006, Train_accy 30.34, Test_accy 4.09
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 21/25 => Loss 2.008, Train_accy 29.62, Test_accy 4.06
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 22/25 => Loss 2.005, Train_accy 29.62, Test_accy 4.05
100%|██████████| 79/79 [01:33<00:00,  1.19s/it]
Task 7, Epoch 23/25 => Loss 2.007, Train_accy 29.80, Test_accy 4.06
100%|██████████| 79/79 [01:33<00:00,  1.18s/it]
Task 7, Epoch 24/25 => Loss 2.000, Train_accy 29.56, Test_accy 4.09
100%|██████████| 79/79 [01:34<00:00,  1.19s/it]
Task 7, Epoch 25/25 => Loss 2.004, Train_accy 29.56, Test_accy 4.08
Cache size: 79
Task 7, Epoch 25/25 => Loss 2.004, Train_accy 29.56, Test_accy 4.08
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.40it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 70 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.44it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 71 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.43it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 72 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.43it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 73 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.43it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 74 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.44it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 75 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.44it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 76 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.43it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 77 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.44it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 78 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.42it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 79 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  0%|          | 0/50 [00:00<?, ?it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.43it/s]
best_loss: 0.20740429663658141
Computing class relations...
Old means shape: (70, 768)
New means shape: (10, 768)
Class relations: [75 71 71 71 71 71 75 71 75 75 71 71 71 71 71 71 71 71 71 71 76 71 71 76
 71 71 71 71 71 71 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77
 77 77 77 77 73 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77 77]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 40000
Feature dataset dimension: 768
Label dataset size: 40000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 80])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 625/625 [00:22<00:00, 27.20batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 7 Elapsed time:  4576.479825019836
Average Accuracy (CNN): 21.85625
Learning on 80-90
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([80, 768])
New weight (Cosine FC) torch.Size([90, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=90, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([80, 5000])
New weight (AC model) torch.Size([90, 5000])
119,229,265 total parameters.
29,140,609 training parameters.
Progressive training for task 8
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 1/25 => Loss 2.283, Train_accy 23.72, Test_accy 2.86
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 2/25 => Loss 2.167, Train_accy 24.62, Test_accy 2.91
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 3/25 => Loss 2.154, Train_accy 25.52, Test_accy 3.02
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 4/25 => Loss 2.148, Train_accy 25.70, Test_accy 3.07
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 5/25 => Loss 2.140, Train_accy 25.80, Test_accy 3.04
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 6/25 => Loss 2.137, Train_accy 26.96, Test_accy 3.11
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 7/25 => Loss 2.124, Train_accy 27.16, Test_accy 3.13
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 8/25 => Loss 2.122, Train_accy 27.58, Test_accy 3.14
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 9/25 => Loss 2.123, Train_accy 27.78, Test_accy 3.20
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 10/25 => Loss 2.113, Train_accy 27.32, Test_accy 3.26
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 11/25 => Loss 2.107, Train_accy 28.80, Test_accy 3.28
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 12/25 => Loss 2.104, Train_accy 27.82, Test_accy 3.31
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 13/25 => Loss 2.096, Train_accy 29.52, Test_accy 3.36
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 14/25 => Loss 2.094, Train_accy 28.86, Test_accy 3.40
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 15/25 => Loss 2.092, Train_accy 29.50, Test_accy 3.39
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 16/25 => Loss 2.098, Train_accy 28.84, Test_accy 3.38
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 17/25 => Loss 2.089, Train_accy 28.54, Test_accy 3.41
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 18/25 => Loss 2.086, Train_accy 28.82, Test_accy 3.42
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 19/25 => Loss 2.089, Train_accy 29.10, Test_accy 3.43
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 20/25 => Loss 2.097, Train_accy 29.28, Test_accy 3.41
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 21/25 => Loss 2.082, Train_accy 29.42, Test_accy 3.41
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 22/25 => Loss 2.086, Train_accy 28.98, Test_accy 3.41
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 8, Epoch 23/25 => Loss 2.073, Train_accy 29.16, Test_accy 3.41
100%|██████████| 79/79 [01:37<00:00,  1.23s/it]
Task 8, Epoch 24/25 => Loss 2.086, Train_accy 28.80, Test_accy 3.40
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 8, Epoch 25/25 => Loss 2.087, Train_accy 28.62, Test_accy 3.40
Cache size: 79
Task 8, Epoch 25/25 => Loss 2.087, Train_accy 28.62, Test_accy 3.40
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 80 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.39it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 81 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.40it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 82 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.39it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 83 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.39it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 84 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.39it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 85 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.39it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 86 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.39it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 87 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.41it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 88 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.40it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 89 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  0%|          | 0/50 [00:00<?, ?it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.41it/s]
best_loss: 0.15028690564632416
Computing class relations...
Old means shape: (80, 768)
New means shape: (10, 768)
Class relations: [86 86 86 86 87 87 86 87 86 86 87 87 87 87 87 87 87 87 87 87 81 87 87 87
 87 87 87 87 81 81 84 82 82 89 82 87 82 82 82 82 83 88 88 88 81 84 85 88
 83 83 88 81 87 88 81 88 88 88 88 87 84 84 84 84 84 84 84 84 84 84 89 87
 84 84 84 83 84 89 84 89]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 45000
Feature dataset dimension: 768
Label dataset size: 45000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 90])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 704/704 [00:25<00:00, 27.18batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 8 Elapsed time:  4988.470829248428
Average Accuracy (CNN): 20.016666666666666
Learning on 90-100
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([90, 768])
New weight (Cosine FC) torch.Size([100, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=100, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([90, 5000])
New weight (AC model) torch.Size([100, 5000])
119,286,945 total parameters.
29,148,289 training parameters.
Progressive training for task 9
100%|██████████| 79/79 [01:38<00:00,  1.24s/it]
Task 9, Epoch 1/25 => Loss 2.070, Train_accy 30.84, Test_accy 3.30
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 9, Epoch 2/25 => Loss 1.910, Train_accy 31.78, Test_accy 3.35
100%|██████████| 79/79 [01:38<00:00,  1.24s/it]
Task 9, Epoch 3/25 => Loss 1.885, Train_accy 33.08, Test_accy 3.52
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 9, Epoch 4/25 => Loss 1.857, Train_accy 33.30, Test_accy 3.58
100%|██████████| 79/79 [01:38<00:00,  1.24s/it]
Task 9, Epoch 5/25 => Loss 1.843, Train_accy 34.38, Test_accy 3.62
100%|██████████| 79/79 [01:38<00:00,  1.24s/it]
Task 9, Epoch 6/25 => Loss 1.819, Train_accy 34.90, Test_accy 3.74
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 9, Epoch 7/25 => Loss 1.809, Train_accy 34.58, Test_accy 3.81
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 9, Epoch 8/25 => Loss 1.789, Train_accy 34.92, Test_accy 3.85
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 9, Epoch 9/25 => Loss 1.782, Train_accy 35.92, Test_accy 3.88
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 9, Epoch 10/25 => Loss 1.779, Train_accy 35.30, Test_accy 3.86
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 9, Epoch 11/25 => Loss 1.770, Train_accy 35.44, Test_accy 3.89
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 9, Epoch 12/25 => Loss 1.755, Train_accy 37.06, Test_accy 3.90
100%|██████████| 79/79 [01:38<00:00,  1.24s/it]
Task 9, Epoch 13/25 => Loss 1.752, Train_accy 35.92, Test_accy 3.86
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 9, Epoch 14/25 => Loss 1.748, Train_accy 36.12, Test_accy 3.88
100%|██████████| 79/79 [01:38<00:00,  1.24s/it]
Task 9, Epoch 15/25 => Loss 1.744, Train_accy 36.76, Test_accy 3.87
100%|██████████| 79/79 [01:37<00:00,  1.24s/it]
Task 9, Epoch 16/25 => Loss 1.746, Train_accy 36.00, Test_accy 3.89
100%|██████████| 79/79 [01:38<00:00,  1.24s/it]
Task 9, Epoch 17/25 => Loss 1.743, Train_accy 36.20, Test_accy 3.88
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 9, Epoch 18/25 => Loss 1.742, Train_accy 36.98, Test_accy 3.84
100%|██████████| 79/79 [01:38<00:00,  1.24s/it]
Task 9, Epoch 19/25 => Loss 1.732, Train_accy 36.48, Test_accy 3.86
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 9, Epoch 20/25 => Loss 1.740, Train_accy 36.56, Test_accy 3.87
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 9, Epoch 21/25 => Loss 1.743, Train_accy 36.66, Test_accy 3.87
100%|██████████| 79/79 [01:38<00:00,  1.24s/it]
Task 9, Epoch 22/25 => Loss 1.738, Train_accy 37.18, Test_accy 3.88
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 9, Epoch 23/25 => Loss 1.729, Train_accy 36.36, Test_accy 3.87
100%|██████████| 79/79 [01:38<00:00,  1.25s/it]
Task 9, Epoch 24/25 => Loss 1.731, Train_accy 36.40, Test_accy 3.87
100%|██████████| 79/79 [01:38<00:00,  1.24s/it]
Task 9, Epoch 25/25 => Loss 1.735, Train_accy 36.50, Test_accy 3.87
Cache size: 79
Task 9, Epoch 25/25 => Loss 1.735, Train_accy 36.50, Test_accy 3.87
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:06<00:00,  1.31it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 90 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.35it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 91 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 92 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 93 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 94 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 95 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.35it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 96 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 97 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.35it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 98 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.36it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 99 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  0%|          | 0/50 [00:00<?, ?it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.42it/s]
best_loss: 0.1994277551174164
Computing class relations...
Old means shape: (90, 768)
New means shape: (10, 768)
Class relations: [95 94 94 94 94 94 93 94 94 93 94 97 94 94 94 94 94 94 94 94 93 93 93 93
 93 93 93 93 93 93 91 91 91 95 95 91 91 91 91 91 94 94 94 94 94 94 94 94
 94 94 93 94 94 94 93 94 93 94 93 94 94 91 91 91 91 91 91 91 91 91 91 94
 91 91 91 91 91 91 91 91 96 92 96 90 98 98 93 96 98 98]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 50000
Feature dataset dimension: 768
Label dataset size: 50000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95
 96 97 98 99]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 100])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 782/782 [00:28<00:00, 27.21batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 9 Elapsed time:  5400.654026508331
Average Accuracy (CNN): 18.531