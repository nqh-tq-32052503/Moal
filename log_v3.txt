100%|██████████| 79/79 [01:16<00:00,  1.03it/s]
Task 0, Epoch 1/20 => Loss 2.269, Train_accy 25.42, Test_accy 46.70
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 2/20 => Loss 2.156, Train_accy 42.70, Test_accy 50.20
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 3/20 => Loss 1.985, Train_accy 47.46, Test_accy 48.90
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 4/20 => Loss 1.836, Train_accy 48.00, Test_accy 52.00
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 5/20 => Loss 1.714, Train_accy 49.44, Test_accy 56.50
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 6/20 => Loss 1.609, Train_accy 52.70, Test_accy 58.30
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 7/20 => Loss 1.544, Train_accy 53.82, Test_accy 63.60
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 8/20 => Loss 1.475, Train_accy 55.76, Test_accy 64.50
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 9/20 => Loss 1.415, Train_accy 58.14, Test_accy 65.70
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 10/20 => Loss 1.376, Train_accy 59.22, Test_accy 67.80
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 11/20 => Loss 1.338, Train_accy 60.70, Test_accy 69.40
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 12/20 => Loss 1.305, Train_accy 60.88, Test_accy 70.30
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 13/20 => Loss 1.280, Train_accy 61.98, Test_accy 71.00
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 14/20 => Loss 1.248, Train_accy 62.38, Test_accy 72.10
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 15/20 => Loss 1.244, Train_accy 62.88, Test_accy 73.50
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 16/20 => Loss 1.229, Train_accy 63.00, Test_accy 74.20
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 17/20 => Loss 1.216, Train_accy 64.52, Test_accy 74.20
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 18/20 => Loss 1.208, Train_accy 64.36, Test_accy 74.70
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 19/20 => Loss 1.205, Train_accy 64.84, Test_accy 75.00
100%|██████████| 79/79 [01:15<00:00,  1.04it/s]
Task 0, Epoch 20/20 => Loss 1.198, Train_accy 65.20, Test_accy 75.10
This is for the BaseNet initialization.
After BaseNet initialization.
Clear the backbone in MultiBranchCosineIncrementalNet, since we are using self.backbones with dual branches
Constructed dual branch network.
New network structure:
MultiBranchCosineIncrementalNet_adapt_AC(
  (backbone): Identity()
  (backbones): ModuleList(
    (0): VisionTransformerBiLoRA(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): Sequential(
        (0): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (8): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (9): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (10): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (11): BlockBiLoRA(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention_FFT(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (lora_A_k): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_k): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (lora_A_v): ModuleList(
              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)
            )
            (lora_B_v): ModuleList(
              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)
            )
            (coef_k): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
            (coef_v): ParameterList(
                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]
            )
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (pre_logits): Identity()
      (fc_norm): Identity()
      (head): Identity()
    )
  )
  (fc): CosineLinear2()
)
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=10, bias=False)
  )
)

Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.95it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 0 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.95it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 1 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.94it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 2 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.95it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 3 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.94it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 4 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.94it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 5 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.94it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 6 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.95it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 7 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.95it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 8 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.95it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 9 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Starting class alignment...
Alignment: 100%|██████████| 79/79 [00:37<00:00,  2.12batch/s]
Embedding shape:  torch.Size([5000, 5000])
Label shape torch.Size([5000])
One-hot label shape:  torch.Size([5000, 10])
Optimising ridge parameter...
selected lambda = 1000.0
gamma 1000.0
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 0 Elapsed time:  1782.7957866191864
Average Accuracy (CNN): 81.4
Learning on 10-20
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([10, 768])
New weight (Cosine FC) torch.Size([20, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=20, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([10, 5000])
New weight (AC model) torch.Size([20, 5000])
118,825,505 total parameters.
29,086,849 training parameters.
Progressive training for task 1
100%|██████████| 79/79 [01:18<00:00,  1.01it/s]
Task 1, Epoch 1/15 => Loss 2.478, Train_accy 22.48, Test_accy 11.30
100%|██████████| 79/79 [01:18<00:00,  1.01it/s]
Task 1, Epoch 2/15 => Loss 2.247, Train_accy 23.54, Test_accy 11.90
100%|██████████| 79/79 [01:18<00:00,  1.00it/s]
Task 1, Epoch 3/15 => Loss 2.189, Train_accy 25.08, Test_accy 12.25
100%|██████████| 79/79 [01:18<00:00,  1.01it/s]
Task 1, Epoch 4/15 => Loss 2.151, Train_accy 25.40, Test_accy 12.60
100%|██████████| 79/79 [01:18<00:00,  1.01it/s]
Task 1, Epoch 5/15 => Loss 2.120, Train_accy 26.92, Test_accy 13.15
100%|██████████| 79/79 [01:18<00:00,  1.01it/s]
Task 1, Epoch 6/15 => Loss 2.100, Train_accy 27.10, Test_accy 14.05
100%|██████████| 79/79 [01:18<00:00,  1.01it/s]
Task 1, Epoch 7/15 => Loss 2.071, Train_accy 28.40, Test_accy 14.20
100%|██████████| 79/79 [01:18<00:00,  1.01it/s]
Task 1, Epoch 8/15 => Loss 2.058, Train_accy 28.46, Test_accy 14.95
100%|██████████| 79/79 [01:18<00:00,  1.01it/s]
Task 1, Epoch 9/15 => Loss 2.042, Train_accy 30.14, Test_accy 15.15
100%|██████████| 79/79 [01:18<00:00,  1.01it/s]
Task 1, Epoch 10/15 => Loss 2.038, Train_accy 29.74, Test_accy 15.35
100%|██████████| 79/79 [01:18<00:00,  1.01it/s]
Task 1, Epoch 11/15 => Loss 2.026, Train_accy 29.76, Test_accy 15.50
100%|██████████| 79/79 [01:18<00:00,  1.01it/s]
Task 1, Epoch 12/15 => Loss 2.019, Train_accy 30.88, Test_accy 15.70
100%|██████████| 79/79 [01:18<00:00,  1.01it/s]
Task 1, Epoch 13/15 => Loss 2.017, Train_accy 31.20, Test_accy 15.75
100%|██████████| 79/79 [01:18<00:00,  1.01it/s]
Task 1, Epoch 14/15 => Loss 2.014, Train_accy 31.24, Test_accy 15.90
100%|██████████| 79/79 [01:18<00:00,  1.01it/s]
Task 1, Epoch 15/15 => Loss 2.012, Train_accy 31.00, Test_accy 15.90
Cache size: 79
Task 1, Epoch 15/15 => Loss 2.012, Train_accy 31.00, Test_accy 15.90
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.85it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 10 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.85it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 11 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.84it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 12 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.86it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 13 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.85it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 14 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.84it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 15 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.84it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 16 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.85it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 17 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.84it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 18 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.86it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 19 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  0%|          | 0/50 [00:00<?, ?it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.36it/s]
best_loss: 0.7467939701080323
Computing class relations...
Old means shape: (10, 768)
New means shape: (10, 768)
Class relations: [10 12 12 12 11 16 10 14 10 10]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 10000
Feature dataset dimension: 768
Label dataset size: 10000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 20])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 157/157 [00:05<00:00, 27.50batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 1 Elapsed time:  1532.615850687027
Average Accuracy (CNN): 55.2
Learning on 20-30
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([20, 768])
New weight (Cosine FC) torch.Size([30, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=30, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([20, 5000])
New weight (AC model) torch.Size([30, 5000])
118,883,185 total parameters.
29,094,529 training parameters.
Progressive training for task 2
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 1/15 => Loss 2.597, Train_accy 16.84, Test_accy 5.70
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 2/15 => Loss 2.493, Train_accy 15.66, Test_accy 5.33
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 3/15 => Loss 2.449, Train_accy 15.42, Test_accy 5.47
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 4/15 => Loss 2.420, Train_accy 15.68, Test_accy 5.63
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 5/15 => Loss 2.395, Train_accy 16.12, Test_accy 5.67
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 6/15 => Loss 2.376, Train_accy 15.74, Test_accy 5.60
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 7/15 => Loss 2.360, Train_accy 15.76, Test_accy 5.67
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 8/15 => Loss 2.347, Train_accy 16.12, Test_accy 5.73
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 9/15 => Loss 2.339, Train_accy 16.16, Test_accy 5.67
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 10/15 => Loss 2.332, Train_accy 15.88, Test_accy 5.63
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 11/15 => Loss 2.322, Train_accy 16.28, Test_accy 5.70
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 12/15 => Loss 2.320, Train_accy 16.84, Test_accy 5.77
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 13/15 => Loss 2.315, Train_accy 16.72, Test_accy 5.90
100%|██████████| 79/79 [01:20<00:00,  1.02s/it]
Task 2, Epoch 14/15 => Loss 2.320, Train_accy 16.66, Test_accy 5.90
100%|██████████| 79/79 [01:22<00:00,  1.04s/it]
Task 2, Epoch 15/15 => Loss 2.319, Train_accy 16.62, Test_accy 5.90
Cache size: 79
Task 2, Epoch 15/15 => Loss 2.319, Train_accy 16.62, Test_accy 5.90
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.72it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 20 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.72it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 21 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.73it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 22 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.73it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 23 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.72it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 24 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.72it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 25 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.71it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 26 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.73it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 27 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.73it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 28 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.73it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 29 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  2%|▏         | 1/50 [00:00<00:09,  5.17it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.55it/s]
best_loss: 0.4194722945690155
Computing class relations...
Old means shape: (20, 768)
New means shape: (10, 768)
Class relations: [23 23 23 23 29 24 23 24 23 22 23 24 24 23 24 23 23 23 23 23]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 15000
Feature dataset dimension: 768
Label dataset size: 15000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 30])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 235/235 [00:08<00:00, 26.70batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 2 Elapsed time:  1714.2930116653442
Average Accuracy (CNN): 42.02333333333333
Learning on 30-40
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([30, 768])
New weight (Cosine FC) torch.Size([40, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=40, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([30, 5000])
New weight (AC model) torch.Size([40, 5000])
118,940,865 total parameters.
29,102,209 training parameters.
Progressive training for task 3
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 1/15 => Loss 2.204, Train_accy 30.48, Test_accy 8.40
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 2/15 => Loss 2.110, Train_accy 31.52, Test_accy 8.57
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 3/15 => Loss 2.079, Train_accy 31.42, Test_accy 8.70
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 4/15 => Loss 2.046, Train_accy 32.34, Test_accy 8.72
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 5/15 => Loss 2.020, Train_accy 32.30, Test_accy 8.93
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 6/15 => Loss 2.009, Train_accy 32.74, Test_accy 9.00
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 7/15 => Loss 1.999, Train_accy 32.82, Test_accy 9.18
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 8/15 => Loss 1.986, Train_accy 33.38, Test_accy 9.28
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 9/15 => Loss 1.977, Train_accy 33.54, Test_accy 9.30
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 10/15 => Loss 1.977, Train_accy 34.18, Test_accy 9.43
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 11/15 => Loss 1.969, Train_accy 34.44, Test_accy 9.45
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 12/15 => Loss 1.965, Train_accy 33.96, Test_accy 9.52
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 13/15 => Loss 1.975, Train_accy 34.12, Test_accy 9.52
100%|██████████| 79/79 [01:23<00:00,  1.06s/it]
Task 3, Epoch 14/15 => Loss 1.966, Train_accy 34.38, Test_accy 9.52
100%|██████████| 79/79 [01:27<00:00,  1.11s/it]
Task 3, Epoch 15/15 => Loss 1.966, Train_accy 34.46, Test_accy 9.52
Cache size: 79
Task 3, Epoch 15/15 => Loss 1.966, Train_accy 34.46, Test_accy 9.52
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.64it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 30 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.64it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 31 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.64it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 32 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.64it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 33 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.64it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 34 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.64it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 35 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.63it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 36 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.64it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 37 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.65it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 38 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:04<00:00,  1.65it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 39 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  0%|          | 0/50 [00:00<?, ?it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.41it/s]
best_loss: 0.3685712623596191
Computing class relations...
Old means shape: (30, 768)
New means shape: (10, 768)
Class relations: [35 30 30 38 30 30 35 39 30 30 38 31 38 30 35 38 38 38 38 38 34 39 39 39
 39 34 34 34 34 34]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 20000
Feature dataset dimension: 768
Label dataset size: 20000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 40])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 313/313 [00:11<00:00, 27.01batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 3 Elapsed time:  1925.1477491855621
Average Accuracy (CNN): 34.8925
Learning on 40-50
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([40, 768])
New weight (Cosine FC) torch.Size([50, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=50, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([40, 5000])
New weight (AC model) torch.Size([50, 5000])
118,998,545 total parameters.
29,109,889 training parameters.
Progressive training for task 4
100%|██████████| 79/79 [01:26<00:00,  1.09s/it]
Task 4, Epoch 1/15 => Loss 2.121, Train_accy 32.12, Test_accy 7.06
100%|██████████| 79/79 [01:25<00:00,  1.09s/it]
Task 4, Epoch 2/15 => Loss 2.043, Train_accy 33.12, Test_accy 7.22
100%|██████████| 79/79 [01:25<00:00,  1.09s/it]
Task 4, Epoch 3/15 => Loss 2.012, Train_accy 33.92, Test_accy 7.24
100%|██████████| 79/79 [01:25<00:00,  1.09s/it]
Task 4, Epoch 4/15 => Loss 2.001, Train_accy 32.94, Test_accy 7.24
100%|██████████| 79/79 [01:25<00:00,  1.09s/it]
Task 4, Epoch 5/15 => Loss 1.984, Train_accy 33.98, Test_accy 7.18
100%|██████████| 79/79 [01:26<00:00,  1.09s/it]
Task 4, Epoch 6/15 => Loss 1.971, Train_accy 33.72, Test_accy 7.26
100%|██████████| 79/79 [01:25<00:00,  1.09s/it]
Task 4, Epoch 7/15 => Loss 1.953, Train_accy 34.30, Test_accy 7.26
100%|██████████| 79/79 [01:25<00:00,  1.08s/it]
Task 4, Epoch 8/15 => Loss 1.943, Train_accy 34.90, Test_accy 7.26
100%|██████████| 79/79 [01:25<00:00,  1.09s/it]
Task 4, Epoch 9/15 => Loss 1.943, Train_accy 34.32, Test_accy 7.30
100%|██████████| 79/79 [01:25<00:00,  1.09s/it]
Task 4, Epoch 10/15 => Loss 1.942, Train_accy 34.76, Test_accy 7.30
100%|██████████| 79/79 [01:25<00:00,  1.08s/it]
Task 4, Epoch 11/15 => Loss 1.935, Train_accy 34.86, Test_accy 7.34
100%|██████████| 79/79 [01:25<00:00,  1.08s/it]
Task 4, Epoch 12/15 => Loss 1.933, Train_accy 34.84, Test_accy 7.32
100%|██████████| 79/79 [01:25<00:00,  1.09s/it]
Task 4, Epoch 13/15 => Loss 1.936, Train_accy 34.62, Test_accy 7.34
100%|██████████| 79/79 [01:25<00:00,  1.08s/it]
Task 4, Epoch 14/15 => Loss 1.925, Train_accy 35.26, Test_accy 7.34
100%|██████████| 79/79 [01:26<00:00,  1.09s/it]
Task 4, Epoch 15/15 => Loss 1.926, Train_accy 34.94, Test_accy 7.34
Cache size: 79
Task 4, Epoch 15/15 => Loss 1.926, Train_accy 34.94, Test_accy 7.34
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.59it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 40 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.55it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 41 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.59it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 42 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.57it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 43 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.59it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 44 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.56it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 45 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.59it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 46 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.58it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 47 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.58it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 48 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.58it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 49 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  2%|▏         | 1/50 [00:00<00:09,  5.14it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.48it/s]
best_loss: 0.1956224697828293
Computing class relations...
Old means shape: (40, 768)
New means shape: (10, 768)
Class relations: [46 48 49 48 45 49 49 48 49 49 49 49 49 49 49 49 49 49 49 49 46 46 46 46
 46 46 46 46 46 46 45 40 41 44 47 46 48 45 45 45]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 25000
Feature dataset dimension: 768
Label dataset size: 25000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 50])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 391/391 [00:14<00:00, 27.07batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 4 Elapsed time:  2125.983263731003
Average Accuracy (CNN): 30.101999999999997
Learning on 50-60
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([50, 768])
New weight (Cosine FC) torch.Size([60, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=60, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([50, 5000])
New weight (AC model) torch.Size([60, 5000])
119,056,225 total parameters.
29,117,569 training parameters.
Progressive training for task 5
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 1/15 => Loss 2.284, Train_accy 20.72, Test_accy 3.55
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 2/15 => Loss 2.186, Train_accy 21.88, Test_accy 3.72
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 3/15 => Loss 2.178, Train_accy 21.94, Test_accy 3.83
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 4/15 => Loss 2.166, Train_accy 22.82, Test_accy 3.93
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 5/15 => Loss 2.163, Train_accy 23.08, Test_accy 3.98
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 6/15 => Loss 2.151, Train_accy 23.20, Test_accy 4.10
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 7/15 => Loss 2.143, Train_accy 23.72, Test_accy 4.15
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 8/15 => Loss 2.142, Train_accy 23.80, Test_accy 4.15
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 9/15 => Loss 2.133, Train_accy 24.68, Test_accy 4.18
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 10/15 => Loss 2.136, Train_accy 24.34, Test_accy 4.18
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 11/15 => Loss 2.127, Train_accy 24.48, Test_accy 4.18
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 12/15 => Loss 2.125, Train_accy 25.04, Test_accy 4.17
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 13/15 => Loss 2.120, Train_accy 25.80, Test_accy 4.17
100%|██████████| 79/79 [01:28<00:00,  1.12s/it]
Task 5, Epoch 14/15 => Loss 2.122, Train_accy 24.42, Test_accy 4.17
100%|██████████| 79/79 [01:29<00:00,  1.13s/it]
Task 5, Epoch 15/15 => Loss 2.126, Train_accy 24.82, Test_accy 4.17
Cache size: 79
Task 5, Epoch 15/15 => Loss 2.126, Train_accy 24.82, Test_accy 4.17
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.51it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 50 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.54it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 51 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.54it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 52 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.53it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 53 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.53it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 54 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.54it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 55 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.53it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 56 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.53it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 57 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.53it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 58 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.53it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 59 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  0%|          | 0/50 [00:00<?, ?it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.30it/s]
best_loss: 0.2195777723789215
Computing class relations...
Old means shape: (50, 768)
New means shape: (10, 768)
Class relations: [53 51 51 51 51 54 53 53 53 53 51 51 51 53 53 51 51 51 51 53 51 50 51 50
 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 51 53 53 53 51 51 53 53
 53 50]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 30000
Feature dataset dimension: 768
Label dataset size: 30000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 60])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 469/469 [00:17<00:00, 27.08batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 5 Elapsed time:  2336.3894443511963
Average Accuracy (CNN): 26.56
Learning on 60-70
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([60, 768])
New weight (Cosine FC) torch.Size([70, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=70, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([60, 5000])
New weight (AC model) torch.Size([70, 5000])
119,113,905 total parameters.
29,125,249 training parameters.
Progressive training for task 6
100%|██████████| 79/79 [01:30<00:00,  1.15s/it]
Task 6, Epoch 1/15 => Loss 4.026, Train_accy 20.06, Test_accy 3.11
100%|██████████| 79/79 [01:30<00:00,  1.15s/it]
Task 6, Epoch 2/15 => Loss 3.551, Train_accy 20.76, Test_accy 3.10
100%|██████████| 79/79 [01:30<00:00,  1.15s/it]
Task 6, Epoch 3/15 => Loss 3.256, Train_accy 20.56, Test_accy 2.99
100%|██████████| 79/79 [01:30<00:00,  1.15s/it]
Task 6, Epoch 4/15 => Loss 3.143, Train_accy 20.94, Test_accy 2.96
100%|██████████| 79/79 [01:30<00:00,  1.15s/it]
Task 6, Epoch 5/15 => Loss 3.057, Train_accy 20.12, Test_accy 2.89
100%|██████████| 79/79 [01:30<00:00,  1.15s/it]
Task 6, Epoch 6/15 => Loss 3.000, Train_accy 20.64, Test_accy 2.91
100%|██████████| 79/79 [01:30<00:00,  1.15s/it]
Task 6, Epoch 7/15 => Loss 2.951, Train_accy 19.68, Test_accy 2.91
100%|██████████| 79/79 [01:30<00:00,  1.15s/it]
Task 6, Epoch 8/15 => Loss 2.912, Train_accy 20.56, Test_accy 2.90
100%|██████████| 79/79 [01:30<00:00,  1.14s/it]
Task 6, Epoch 9/15 => Loss 2.899, Train_accy 20.12, Test_accy 2.94
100%|██████████| 79/79 [01:30<00:00,  1.15s/it]
Task 6, Epoch 10/15 => Loss 2.851, Train_accy 20.24, Test_accy 2.86
100%|██████████| 79/79 [01:30<00:00,  1.14s/it]
Task 6, Epoch 11/15 => Loss 2.855, Train_accy 19.56, Test_accy 2.84
100%|██████████| 79/79 [01:30<00:00,  1.14s/it]
Task 6, Epoch 12/15 => Loss 2.817, Train_accy 20.40, Test_accy 2.84
100%|██████████| 79/79 [01:30<00:00,  1.14s/it]
Task 6, Epoch 13/15 => Loss 2.821, Train_accy 19.00, Test_accy 2.86
100%|██████████| 79/79 [01:30<00:00,  1.15s/it]
Task 6, Epoch 14/15 => Loss 2.822, Train_accy 19.88, Test_accy 2.84
100%|██████████| 79/79 [01:30<00:00,  1.15s/it]
Task 6, Epoch 15/15 => Loss 2.805, Train_accy 20.00, Test_accy 2.84
Cache size: 79
Task 6, Epoch 15/15 => Loss 2.805, Train_accy 20.00, Test_accy 2.84
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.47it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 60 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.47it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 61 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.49it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 62 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.48it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 63 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.48it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 64 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.49it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 65 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.50it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 66 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.49it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 67 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.48it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 68 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.48it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 69 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  2%|▏         | 1/50 [00:00<00:09,  5.05it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.48it/s]
best_loss: 0.20283790159225465
Computing class relations...
Old means shape: (60, 768)
New means shape: (10, 768)
Class relations: [60 66 66 62 60 69 68 69 60 60 69 69 69 69 69 69 69 69 69 69 69 69 69 69
 69 69 69 69 69 69 69 66 69 69 69 69 69 69 69 69 66 62 62 69 62 62 69 69
 62 69 64 68 64 69 64 64 67 64 64 64]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 35000
Feature dataset dimension: 768
Label dataset size: 35000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 70])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 547/547 [00:20<00:00, 27.04batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 6 Elapsed time:  2560.994246006012
Average Accuracy (CNN): 23.627142857142854
Learning on 70-80
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([70, 768])
New weight (Cosine FC) torch.Size([80, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=80, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([70, 5000])
New weight (AC model) torch.Size([80, 5000])
119,171,585 total parameters.
29,132,929 training parameters.
Progressive training for task 7
100%|██████████| 79/79 [01:33<00:00,  1.18s/it]
Task 7, Epoch 1/15 => Loss 2.342, Train_accy 25.90, Test_accy 3.72
100%|██████████| 79/79 [01:33<00:00,  1.18s/it]
Task 7, Epoch 2/15 => Loss 2.209, Train_accy 26.72, Test_accy 3.71
100%|██████████| 79/79 [01:33<00:00,  1.18s/it]
Task 7, Epoch 3/15 => Loss 2.175, Train_accy 27.52, Test_accy 3.75
100%|██████████| 79/79 [01:32<00:00,  1.17s/it]
Task 7, Epoch 4/15 => Loss 2.150, Train_accy 27.32, Test_accy 3.82
100%|██████████| 79/79 [01:32<00:00,  1.17s/it]
Task 7, Epoch 5/15 => Loss 2.138, Train_accy 28.44, Test_accy 3.76
100%|██████████| 79/79 [01:32<00:00,  1.17s/it]
Task 7, Epoch 6/15 => Loss 2.133, Train_accy 27.02, Test_accy 3.78
100%|██████████| 79/79 [01:32<00:00,  1.17s/it]
Task 7, Epoch 7/15 => Loss 2.122, Train_accy 27.50, Test_accy 3.74
100%|██████████| 79/79 [01:32<00:00,  1.17s/it]
Task 7, Epoch 8/15 => Loss 2.116, Train_accy 26.66, Test_accy 3.78
100%|██████████| 79/79 [01:32<00:00,  1.17s/it]
Task 7, Epoch 9/15 => Loss 2.112, Train_accy 27.08, Test_accy 3.82
100%|██████████| 79/79 [01:32<00:00,  1.17s/it]
Task 7, Epoch 10/15 => Loss 2.112, Train_accy 27.60, Test_accy 3.84
100%|██████████| 79/79 [01:32<00:00,  1.17s/it]
Task 7, Epoch 11/15 => Loss 2.105, Train_accy 27.38, Test_accy 3.79
100%|██████████| 79/79 [01:32<00:00,  1.17s/it]
Task 7, Epoch 12/15 => Loss 2.103, Train_accy 27.26, Test_accy 3.80
100%|██████████| 79/79 [01:32<00:00,  1.18s/it]
Task 7, Epoch 13/15 => Loss 2.099, Train_accy 27.40, Test_accy 3.81
100%|██████████| 79/79 [01:32<00:00,  1.17s/it]
Task 7, Epoch 14/15 => Loss 2.099, Train_accy 27.70, Test_accy 3.81
100%|██████████| 79/79 [01:36<00:00,  1.23s/it]
Task 7, Epoch 15/15 => Loss 2.095, Train_accy 28.46, Test_accy 3.84
Cache size: 79
Task 7, Epoch 15/15 => Loss 2.095, Train_accy 28.46, Test_accy 3.84
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.40it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 70 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.42it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 71 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.42it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 72 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.42it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 73 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.41it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 74 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.41it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 75 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.41it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 76 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.41it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 77 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.42it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 78 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.42it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 79 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  2%|▏         | 1/50 [00:00<00:09,  5.01it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.44it/s]
best_loss: 0.10507464337348937
Computing class relations...
Old means shape: (70, 768)
New means shape: (10, 768)
Class relations: [78 77 77 76 78 71 78 71 78 78 78 78 78 71 71 78 78 78 78 78 71 71 71 71
 71 71 71 71 71 71 78 77 78 73 71 71 71 73 78 71 75 71 71 71 76 75 71 71
 75 71 71 71 71 71 71 71 71 71 71 71 78 72 76 78 78 70 74 70 72 78]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 40000
Feature dataset dimension: 768
Label dataset size: 40000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 80])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 625/625 [00:23<00:00, 27.02batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 7 Elapsed time:  2797.1790409088135
Average Accuracy (CNN): 21.2675
Learning on 80-90
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([80, 768])
New weight (Cosine FC) torch.Size([90, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=90, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([80, 5000])
New weight (AC model) torch.Size([90, 5000])
119,229,265 total parameters.
29,140,609 training parameters.
Progressive training for task 8
100%|██████████| 79/79 [01:35<00:00,  1.22s/it]
Task 8, Epoch 1/15 => Loss 2.424, Train_accy 23.04, Test_accy 2.84
100%|██████████| 79/79 [01:35<00:00,  1.21s/it]
Task 8, Epoch 2/15 => Loss 2.269, Train_accy 24.04, Test_accy 2.90
100%|██████████| 79/79 [01:35<00:00,  1.21s/it]
Task 8, Epoch 3/15 => Loss 2.243, Train_accy 23.54, Test_accy 2.92
100%|██████████| 79/79 [01:35<00:00,  1.21s/it]
Task 8, Epoch 4/15 => Loss 2.227, Train_accy 23.36, Test_accy 2.98
100%|██████████| 79/79 [01:35<00:00,  1.21s/it]
Task 8, Epoch 5/15 => Loss 2.214, Train_accy 24.14, Test_accy 2.96
100%|██████████| 79/79 [01:35<00:00,  1.21s/it]
Task 8, Epoch 6/15 => Loss 2.205, Train_accy 25.06, Test_accy 3.01
100%|██████████| 79/79 [01:35<00:00,  1.21s/it]
Task 8, Epoch 7/15 => Loss 2.197, Train_accy 24.22, Test_accy 3.00
100%|██████████| 79/79 [01:35<00:00,  1.21s/it]
Task 8, Epoch 8/15 => Loss 2.191, Train_accy 24.10, Test_accy 3.06
100%|██████████| 79/79 [01:35<00:00,  1.21s/it]
Task 8, Epoch 9/15 => Loss 2.190, Train_accy 24.68, Test_accy 3.09
100%|██████████| 79/79 [01:35<00:00,  1.21s/it]
Task 8, Epoch 10/15 => Loss 2.184, Train_accy 25.72, Test_accy 3.09
100%|██████████| 79/79 [01:35<00:00,  1.21s/it]
Task 8, Epoch 11/15 => Loss 2.174, Train_accy 25.42, Test_accy 3.08
100%|██████████| 79/79 [01:35<00:00,  1.21s/it]
Task 8, Epoch 12/15 => Loss 2.177, Train_accy 25.70, Test_accy 3.08
100%|██████████| 79/79 [01:35<00:00,  1.21s/it]
Task 8, Epoch 13/15 => Loss 2.170, Train_accy 25.62, Test_accy 3.09
100%|██████████| 79/79 [01:35<00:00,  1.21s/it]
Task 8, Epoch 14/15 => Loss 2.176, Train_accy 25.68, Test_accy 3.09
100%|██████████| 79/79 [01:35<00:00,  1.20s/it]
Task 8, Epoch 15/15 => Loss 2.172, Train_accy 26.10, Test_accy 3.09
Cache size: 79
Task 8, Epoch 15/15 => Loss 2.172, Train_accy 26.10, Test_accy 3.09
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.38it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 80 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.38it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 81 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.38it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 82 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.39it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 83 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.39it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 84 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.37it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 85 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.38it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 86 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.38it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 87 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.39it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 88 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.38it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 89 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  0%|          | 0/50 [00:00<?, ?it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.39it/s]
best_loss: 0.13666301059722902
Computing class relations...
Old means shape: (80, 768)
New means shape: (10, 768)
Class relations: [81 89 83 89 81 87 81 87 87 81 81 81 81 86 81 81 81 81 86 81 81 81 81 81
 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 81 83 86 86 86 86 86 86 86
 86 86 86 81 86 86 86 86 86 86 86 86 81 81 86 81 86 86 88 84 81 86 84 87
 85 81 84 85 84 89 81 84]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 45000
Feature dataset dimension: 768
Label dataset size: 45000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 90])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 704/704 [00:26<00:00, 27.04batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 8 Elapsed time:  3046.901474714279
Average Accuracy (CNN): 19.397777777777776
Learning on 90-100
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([90, 768])
New weight (Cosine FC) torch.Size([100, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=100, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([90, 5000])
New weight (AC model) torch.Size([100, 5000])
119,286,945 total parameters.
29,148,289 training parameters.
Progressive training for task 9
100%|██████████| 79/79 [01:37<00:00,  1.23s/it]
Task 9, Epoch 1/15 => Loss 2.180, Train_accy 33.66, Test_accy 3.33
100%|██████████| 79/79 [01:37<00:00,  1.24s/it]
Task 9, Epoch 2/15 => Loss 1.985, Train_accy 34.34, Test_accy 3.39
100%|██████████| 79/79 [01:37<00:00,  1.23s/it]
Task 9, Epoch 3/15 => Loss 1.940, Train_accy 35.32, Test_accy 3.46
100%|██████████| 79/79 [01:37<00:00,  1.23s/it]
Task 9, Epoch 4/15 => Loss 1.926, Train_accy 34.92, Test_accy 3.53
100%|██████████| 79/79 [01:37<00:00,  1.23s/it]
Task 9, Epoch 5/15 => Loss 1.897, Train_accy 35.94, Test_accy 3.63
100%|██████████| 79/79 [01:37<00:00,  1.23s/it]
Task 9, Epoch 6/15 => Loss 1.896, Train_accy 36.34, Test_accy 3.67
100%|██████████| 79/79 [01:37<00:00,  1.23s/it]
Task 9, Epoch 7/15 => Loss 1.867, Train_accy 35.70, Test_accy 3.73
100%|██████████| 79/79 [01:37<00:00,  1.23s/it]
Task 9, Epoch 8/15 => Loss 1.859, Train_accy 37.10, Test_accy 3.75
100%|██████████| 79/79 [01:37<00:00,  1.23s/it]
Task 9, Epoch 9/15 => Loss 1.855, Train_accy 36.80, Test_accy 3.77
100%|██████████| 79/79 [01:37<00:00,  1.24s/it]
Task 9, Epoch 10/15 => Loss 1.845, Train_accy 37.12, Test_accy 3.77
100%|██████████| 79/79 [01:37<00:00,  1.23s/it]
Task 9, Epoch 11/15 => Loss 1.834, Train_accy 37.22, Test_accy 3.80
100%|██████████| 79/79 [01:37<00:00,  1.23s/it]
Task 9, Epoch 12/15 => Loss 1.835, Train_accy 37.32, Test_accy 3.80
100%|██████████| 79/79 [01:37<00:00,  1.23s/it]
Task 9, Epoch 13/15 => Loss 1.828, Train_accy 36.70, Test_accy 3.79
100%|██████████| 79/79 [01:37<00:00,  1.23s/it]
Task 9, Epoch 14/15 => Loss 1.836, Train_accy 37.12, Test_accy 3.81
100%|██████████| 79/79 [01:36<00:00,  1.22s/it]
Task 9, Epoch 15/15 => Loss 1.832, Train_accy 37.26, Test_accy 3.81
Cache size: 79
Task 9, Epoch 15/15 => Loss 1.832, Train_accy 37.26, Test_accy 3.81
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:06<00:00,  1.31it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 90 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 91 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 92 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 93 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 94 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 95 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 96 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 97 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.35it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 98 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 99 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  0%|          | 0/50 [00:00<?, ?it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.44it/s]
best_loss: 0.12942341649532318
Computing class relations...
Old means shape: (90, 768)
New means shape: (10, 768)
Class relations: [94 91 94 95 94 92 94 94 94 94 92 92 92 93 94 92 92 92 93 92 92 92 94 92
 92 92 92 94 94 92 94 92 94 92 94 94 94 94 94 94 93 93 93 93 93 93 94 93
 93 93 94 94 94 94 93 94 94 94 93 94 99 90 90 90 90 93 90 98 90 99 98 94
 90 99 98 90 98 98 90 98 98 90 98 90 98 90 93 90 98 98]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 50000
Feature dataset dimension: 768
Label dataset size: 50000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95
 96 97 98 99]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 100])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 782/782 [00:28<00:00, 27.09batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 9 Elapsed time:  3297.454295396805
Average Accuracy (CNN): 17.946999999999996