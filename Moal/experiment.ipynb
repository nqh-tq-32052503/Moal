{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2468e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Moal'...\n",
      "remote: Enumerating objects: 217, done.\u001b[K\n",
      "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
      "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
      "remote: Total 217 (delta 32), reused 32 (delta 25), pack-reused 171 (from 1)\u001b[K\n",
      "Receiving objects: 100% (217/217), 464.31 KiB | 11.91 MiB/s, done.\n",
      "Resolving deltas: 100% (105/105), done.\n",
      "/content/Moal/Moal\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /content/Moal\n",
    "!git clone https://github.com/nqh-tq-32052503/Moal.git\n",
    "%cd /content/Moal/Moal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c1f2b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/root/autodl-tmp/data/cifar224'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4163333044.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/root/autodl-tmp/data/cifar224\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.12/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/root/autodl-tmp/data/cifar224'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"/root/autodl-tmp/data/cifar224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5f3098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"exps/Moal_cifar224_bilora.json\", \"r\") as f:\n",
    "    args = json.load(f)\n",
    "print(type(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9cf4859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_manager import DataManager\n",
    "from utils import factory\n",
    "from utils.toolkit import count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd2ff06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 92, 14, 57, 89, 20, 34, 69, 21, 3, 77, 33, 54, 86, 49, 79, 98, 15, 16, 61, 46, 59, 39, 12, 56, 50, 27, 87, 45, 78, 24, 0, 7, 51, 4, 73, 8, 35, 10, 13, 70, 74, 31, 55, 82, 5, 67, 75, 22, 76, 47, 83, 90, 1, 6, 84, 94, 66, 88, 91, 23, 29, 64, 37, 72, 18, 11, 44, 40, 93, 80, 30, 81, 68, 32, 58, 63, 2, 17, 65, 38, 41, 97, 48, 25, 28, 52, 99, 9, 36, 85, 62, 60, 96, 95, 53, 19, 42, 26, 71]\n"
     ]
    }
   ],
   "source": [
    "data_manager = DataManager(\n",
    "        args[\"dataset\"],\n",
    "        args[\"shuffle\"],\n",
    "        args[\"seed\"],\n",
    "        args[\"init_cls\"],\n",
    "        args[\"increment\"],\n",
    "        args,\n",
    "        \"/root/autodl-tmp/data/cifar224\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "027403ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"nb_classes\"] = data_manager.nb_classes # update args\n",
    "args[\"nb_tasks\"] = data_manager.nb_tasks\n",
    "args[\"model_name\"] = \"adapt_ac_com_sdc_ema_auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62311f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is for the BaseNet initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After BaseNet initialization.\n"
     ]
    }
   ],
   "source": [
    "model = factory.get_model(args[\"model_name\"], args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2842fe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning on 0-10\n",
      "Train dataset size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114,877,825 total parameters.\n",
      "29,079,169 training parameters.\n",
      "backbone.cls_token_grow 3840000\n",
      "backbone.pos_embed_grow 918528\n",
      "backbone.blocks.0.attn.lora_A_k.0.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_k.1.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_k.2.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_k.3.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_k.4.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_k.5.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_k.6.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_k.7.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_k.8.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_k.9.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_k.0.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_k.1.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_k.2.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_k.3.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_k.4.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_k.5.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_k.6.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_k.7.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_k.8.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_k.9.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_v.0.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_v.1.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_v.2.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_v.3.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_v.4.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_v.5.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_v.6.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_v.7.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_v.8.weight 49152\n",
      "backbone.blocks.0.attn.lora_A_v.9.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_v.0.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_v.1.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_v.2.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_v.3.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_v.4.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_v.5.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_v.6.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_v.7.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_v.8.weight 49152\n",
      "backbone.blocks.0.attn.lora_B_v.9.weight 49152\n",
      "backbone.blocks.0.attn.coef_k.0 3000\n",
      "backbone.blocks.0.attn.coef_k.1 3000\n",
      "backbone.blocks.0.attn.coef_k.2 3000\n",
      "backbone.blocks.0.attn.coef_k.3 3000\n",
      "backbone.blocks.0.attn.coef_k.4 3000\n",
      "backbone.blocks.0.attn.coef_k.5 3000\n",
      "backbone.blocks.0.attn.coef_k.6 3000\n",
      "backbone.blocks.0.attn.coef_k.7 3000\n",
      "backbone.blocks.0.attn.coef_k.8 3000\n",
      "backbone.blocks.0.attn.coef_k.9 3000\n",
      "backbone.blocks.0.attn.coef_v.0 3000\n",
      "backbone.blocks.0.attn.coef_v.1 3000\n",
      "backbone.blocks.0.attn.coef_v.2 3000\n",
      "backbone.blocks.0.attn.coef_v.3 3000\n",
      "backbone.blocks.0.attn.coef_v.4 3000\n",
      "backbone.blocks.0.attn.coef_v.5 3000\n",
      "backbone.blocks.0.attn.coef_v.6 3000\n",
      "backbone.blocks.0.attn.coef_v.7 3000\n",
      "backbone.blocks.0.attn.coef_v.8 3000\n",
      "backbone.blocks.0.attn.coef_v.9 3000\n",
      "backbone.blocks.1.attn.lora_A_k.0.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_k.1.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_k.2.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_k.3.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_k.4.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_k.5.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_k.6.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_k.7.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_k.8.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_k.9.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_k.0.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_k.1.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_k.2.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_k.3.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_k.4.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_k.5.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_k.6.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_k.7.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_k.8.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_k.9.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_v.0.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_v.1.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_v.2.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_v.3.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_v.4.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_v.5.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_v.6.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_v.7.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_v.8.weight 49152\n",
      "backbone.blocks.1.attn.lora_A_v.9.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_v.0.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_v.1.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_v.2.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_v.3.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_v.4.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_v.5.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_v.6.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_v.7.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_v.8.weight 49152\n",
      "backbone.blocks.1.attn.lora_B_v.9.weight 49152\n",
      "backbone.blocks.1.attn.coef_k.0 3000\n",
      "backbone.blocks.1.attn.coef_k.1 3000\n",
      "backbone.blocks.1.attn.coef_k.2 3000\n",
      "backbone.blocks.1.attn.coef_k.3 3000\n",
      "backbone.blocks.1.attn.coef_k.4 3000\n",
      "backbone.blocks.1.attn.coef_k.5 3000\n",
      "backbone.blocks.1.attn.coef_k.6 3000\n",
      "backbone.blocks.1.attn.coef_k.7 3000\n",
      "backbone.blocks.1.attn.coef_k.8 3000\n",
      "backbone.blocks.1.attn.coef_k.9 3000\n",
      "backbone.blocks.1.attn.coef_v.0 3000\n",
      "backbone.blocks.1.attn.coef_v.1 3000\n",
      "backbone.blocks.1.attn.coef_v.2 3000\n",
      "backbone.blocks.1.attn.coef_v.3 3000\n",
      "backbone.blocks.1.attn.coef_v.4 3000\n",
      "backbone.blocks.1.attn.coef_v.5 3000\n",
      "backbone.blocks.1.attn.coef_v.6 3000\n",
      "backbone.blocks.1.attn.coef_v.7 3000\n",
      "backbone.blocks.1.attn.coef_v.8 3000\n",
      "backbone.blocks.1.attn.coef_v.9 3000\n",
      "backbone.blocks.2.attn.lora_A_k.0.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_k.1.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_k.2.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_k.3.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_k.4.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_k.5.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_k.6.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_k.7.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_k.8.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_k.9.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_k.0.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_k.1.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_k.2.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_k.3.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_k.4.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_k.5.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_k.6.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_k.7.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_k.8.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_k.9.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_v.0.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_v.1.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_v.2.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_v.3.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_v.4.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_v.5.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_v.6.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_v.7.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_v.8.weight 49152\n",
      "backbone.blocks.2.attn.lora_A_v.9.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_v.0.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_v.1.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_v.2.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_v.3.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_v.4.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_v.5.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_v.6.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_v.7.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_v.8.weight 49152\n",
      "backbone.blocks.2.attn.lora_B_v.9.weight 49152\n",
      "backbone.blocks.2.attn.coef_k.0 3000\n",
      "backbone.blocks.2.attn.coef_k.1 3000\n",
      "backbone.blocks.2.attn.coef_k.2 3000\n",
      "backbone.blocks.2.attn.coef_k.3 3000\n",
      "backbone.blocks.2.attn.coef_k.4 3000\n",
      "backbone.blocks.2.attn.coef_k.5 3000\n",
      "backbone.blocks.2.attn.coef_k.6 3000\n",
      "backbone.blocks.2.attn.coef_k.7 3000\n",
      "backbone.blocks.2.attn.coef_k.8 3000\n",
      "backbone.blocks.2.attn.coef_k.9 3000\n",
      "backbone.blocks.2.attn.coef_v.0 3000\n",
      "backbone.blocks.2.attn.coef_v.1 3000\n",
      "backbone.blocks.2.attn.coef_v.2 3000\n",
      "backbone.blocks.2.attn.coef_v.3 3000\n",
      "backbone.blocks.2.attn.coef_v.4 3000\n",
      "backbone.blocks.2.attn.coef_v.5 3000\n",
      "backbone.blocks.2.attn.coef_v.6 3000\n",
      "backbone.blocks.2.attn.coef_v.7 3000\n",
      "backbone.blocks.2.attn.coef_v.8 3000\n",
      "backbone.blocks.2.attn.coef_v.9 3000\n",
      "backbone.blocks.3.attn.lora_A_k.0.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_k.1.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_k.2.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_k.3.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_k.4.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_k.5.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_k.6.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_k.7.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_k.8.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_k.9.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_k.0.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_k.1.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_k.2.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_k.3.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_k.4.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_k.5.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_k.6.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_k.7.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_k.8.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_k.9.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_v.0.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_v.1.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_v.2.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_v.3.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_v.4.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_v.5.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_v.6.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_v.7.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_v.8.weight 49152\n",
      "backbone.blocks.3.attn.lora_A_v.9.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_v.0.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_v.1.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_v.2.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_v.3.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_v.4.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_v.5.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_v.6.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_v.7.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_v.8.weight 49152\n",
      "backbone.blocks.3.attn.lora_B_v.9.weight 49152\n",
      "backbone.blocks.3.attn.coef_k.0 3000\n",
      "backbone.blocks.3.attn.coef_k.1 3000\n",
      "backbone.blocks.3.attn.coef_k.2 3000\n",
      "backbone.blocks.3.attn.coef_k.3 3000\n",
      "backbone.blocks.3.attn.coef_k.4 3000\n",
      "backbone.blocks.3.attn.coef_k.5 3000\n",
      "backbone.blocks.3.attn.coef_k.6 3000\n",
      "backbone.blocks.3.attn.coef_k.7 3000\n",
      "backbone.blocks.3.attn.coef_k.8 3000\n",
      "backbone.blocks.3.attn.coef_k.9 3000\n",
      "backbone.blocks.3.attn.coef_v.0 3000\n",
      "backbone.blocks.3.attn.coef_v.1 3000\n",
      "backbone.blocks.3.attn.coef_v.2 3000\n",
      "backbone.blocks.3.attn.coef_v.3 3000\n",
      "backbone.blocks.3.attn.coef_v.4 3000\n",
      "backbone.blocks.3.attn.coef_v.5 3000\n",
      "backbone.blocks.3.attn.coef_v.6 3000\n",
      "backbone.blocks.3.attn.coef_v.7 3000\n",
      "backbone.blocks.3.attn.coef_v.8 3000\n",
      "backbone.blocks.3.attn.coef_v.9 3000\n",
      "backbone.blocks.4.attn.lora_A_k.0.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_k.1.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_k.2.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_k.3.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_k.4.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_k.5.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_k.6.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_k.7.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_k.8.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_k.9.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_k.0.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_k.1.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_k.2.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_k.3.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_k.4.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_k.5.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_k.6.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_k.7.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_k.8.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_k.9.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_v.0.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_v.1.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_v.2.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_v.3.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_v.4.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_v.5.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_v.6.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_v.7.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_v.8.weight 49152\n",
      "backbone.blocks.4.attn.lora_A_v.9.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_v.0.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_v.1.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_v.2.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_v.3.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_v.4.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_v.5.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_v.6.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_v.7.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_v.8.weight 49152\n",
      "backbone.blocks.4.attn.lora_B_v.9.weight 49152\n",
      "backbone.blocks.4.attn.coef_k.0 3000\n",
      "backbone.blocks.4.attn.coef_k.1 3000\n",
      "backbone.blocks.4.attn.coef_k.2 3000\n",
      "backbone.blocks.4.attn.coef_k.3 3000\n",
      "backbone.blocks.4.attn.coef_k.4 3000\n",
      "backbone.blocks.4.attn.coef_k.5 3000\n",
      "backbone.blocks.4.attn.coef_k.6 3000\n",
      "backbone.blocks.4.attn.coef_k.7 3000\n",
      "backbone.blocks.4.attn.coef_k.8 3000\n",
      "backbone.blocks.4.attn.coef_k.9 3000\n",
      "backbone.blocks.4.attn.coef_v.0 3000\n",
      "backbone.blocks.4.attn.coef_v.1 3000\n",
      "backbone.blocks.4.attn.coef_v.2 3000\n",
      "backbone.blocks.4.attn.coef_v.3 3000\n",
      "backbone.blocks.4.attn.coef_v.4 3000\n",
      "backbone.blocks.4.attn.coef_v.5 3000\n",
      "backbone.blocks.4.attn.coef_v.6 3000\n",
      "backbone.blocks.4.attn.coef_v.7 3000\n",
      "backbone.blocks.4.attn.coef_v.8 3000\n",
      "backbone.blocks.4.attn.coef_v.9 3000\n",
      "backbone.blocks.5.attn.lora_A_k.0.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_k.1.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_k.2.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_k.3.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_k.4.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_k.5.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_k.6.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_k.7.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_k.8.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_k.9.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_k.0.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_k.1.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_k.2.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_k.3.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_k.4.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_k.5.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_k.6.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_k.7.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_k.8.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_k.9.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_v.0.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_v.1.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_v.2.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_v.3.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_v.4.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_v.5.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_v.6.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_v.7.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_v.8.weight 49152\n",
      "backbone.blocks.5.attn.lora_A_v.9.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_v.0.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_v.1.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_v.2.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_v.3.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_v.4.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_v.5.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_v.6.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_v.7.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_v.8.weight 49152\n",
      "backbone.blocks.5.attn.lora_B_v.9.weight 49152\n",
      "backbone.blocks.5.attn.coef_k.0 3000\n",
      "backbone.blocks.5.attn.coef_k.1 3000\n",
      "backbone.blocks.5.attn.coef_k.2 3000\n",
      "backbone.blocks.5.attn.coef_k.3 3000\n",
      "backbone.blocks.5.attn.coef_k.4 3000\n",
      "backbone.blocks.5.attn.coef_k.5 3000\n",
      "backbone.blocks.5.attn.coef_k.6 3000\n",
      "backbone.blocks.5.attn.coef_k.7 3000\n",
      "backbone.blocks.5.attn.coef_k.8 3000\n",
      "backbone.blocks.5.attn.coef_k.9 3000\n",
      "backbone.blocks.5.attn.coef_v.0 3000\n",
      "backbone.blocks.5.attn.coef_v.1 3000\n",
      "backbone.blocks.5.attn.coef_v.2 3000\n",
      "backbone.blocks.5.attn.coef_v.3 3000\n",
      "backbone.blocks.5.attn.coef_v.4 3000\n",
      "backbone.blocks.5.attn.coef_v.5 3000\n",
      "backbone.blocks.5.attn.coef_v.6 3000\n",
      "backbone.blocks.5.attn.coef_v.7 3000\n",
      "backbone.blocks.5.attn.coef_v.8 3000\n",
      "backbone.blocks.5.attn.coef_v.9 3000\n",
      "backbone.blocks.6.attn.lora_A_k.0.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_k.1.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_k.2.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_k.3.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_k.4.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_k.5.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_k.6.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_k.7.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_k.8.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_k.9.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_k.0.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_k.1.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_k.2.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_k.3.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_k.4.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_k.5.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_k.6.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_k.7.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_k.8.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_k.9.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_v.0.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_v.1.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_v.2.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_v.3.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_v.4.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_v.5.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_v.6.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_v.7.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_v.8.weight 49152\n",
      "backbone.blocks.6.attn.lora_A_v.9.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_v.0.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_v.1.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_v.2.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_v.3.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_v.4.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_v.5.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_v.6.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_v.7.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_v.8.weight 49152\n",
      "backbone.blocks.6.attn.lora_B_v.9.weight 49152\n",
      "backbone.blocks.6.attn.coef_k.0 3000\n",
      "backbone.blocks.6.attn.coef_k.1 3000\n",
      "backbone.blocks.6.attn.coef_k.2 3000\n",
      "backbone.blocks.6.attn.coef_k.3 3000\n",
      "backbone.blocks.6.attn.coef_k.4 3000\n",
      "backbone.blocks.6.attn.coef_k.5 3000\n",
      "backbone.blocks.6.attn.coef_k.6 3000\n",
      "backbone.blocks.6.attn.coef_k.7 3000\n",
      "backbone.blocks.6.attn.coef_k.8 3000\n",
      "backbone.blocks.6.attn.coef_k.9 3000\n",
      "backbone.blocks.6.attn.coef_v.0 3000\n",
      "backbone.blocks.6.attn.coef_v.1 3000\n",
      "backbone.blocks.6.attn.coef_v.2 3000\n",
      "backbone.blocks.6.attn.coef_v.3 3000\n",
      "backbone.blocks.6.attn.coef_v.4 3000\n",
      "backbone.blocks.6.attn.coef_v.5 3000\n",
      "backbone.blocks.6.attn.coef_v.6 3000\n",
      "backbone.blocks.6.attn.coef_v.7 3000\n",
      "backbone.blocks.6.attn.coef_v.8 3000\n",
      "backbone.blocks.6.attn.coef_v.9 3000\n",
      "backbone.blocks.7.attn.lora_A_k.0.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_k.1.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_k.2.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_k.3.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_k.4.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_k.5.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_k.6.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_k.7.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_k.8.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_k.9.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_k.0.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_k.1.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_k.2.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_k.3.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_k.4.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_k.5.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_k.6.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_k.7.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_k.8.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_k.9.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_v.0.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_v.1.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_v.2.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_v.3.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_v.4.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_v.5.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_v.6.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_v.7.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_v.8.weight 49152\n",
      "backbone.blocks.7.attn.lora_A_v.9.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_v.0.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_v.1.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_v.2.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_v.3.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_v.4.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_v.5.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_v.6.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_v.7.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_v.8.weight 49152\n",
      "backbone.blocks.7.attn.lora_B_v.9.weight 49152\n",
      "backbone.blocks.7.attn.coef_k.0 3000\n",
      "backbone.blocks.7.attn.coef_k.1 3000\n",
      "backbone.blocks.7.attn.coef_k.2 3000\n",
      "backbone.blocks.7.attn.coef_k.3 3000\n",
      "backbone.blocks.7.attn.coef_k.4 3000\n",
      "backbone.blocks.7.attn.coef_k.5 3000\n",
      "backbone.blocks.7.attn.coef_k.6 3000\n",
      "backbone.blocks.7.attn.coef_k.7 3000\n",
      "backbone.blocks.7.attn.coef_k.8 3000\n",
      "backbone.blocks.7.attn.coef_k.9 3000\n",
      "backbone.blocks.7.attn.coef_v.0 3000\n",
      "backbone.blocks.7.attn.coef_v.1 3000\n",
      "backbone.blocks.7.attn.coef_v.2 3000\n",
      "backbone.blocks.7.attn.coef_v.3 3000\n",
      "backbone.blocks.7.attn.coef_v.4 3000\n",
      "backbone.blocks.7.attn.coef_v.5 3000\n",
      "backbone.blocks.7.attn.coef_v.6 3000\n",
      "backbone.blocks.7.attn.coef_v.7 3000\n",
      "backbone.blocks.7.attn.coef_v.8 3000\n",
      "backbone.blocks.7.attn.coef_v.9 3000\n",
      "backbone.blocks.8.attn.lora_A_k.0.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_k.1.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_k.2.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_k.3.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_k.4.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_k.5.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_k.6.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_k.7.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_k.8.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_k.9.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_k.0.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_k.1.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_k.2.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_k.3.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_k.4.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_k.5.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_k.6.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_k.7.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_k.8.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_k.9.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_v.0.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_v.1.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_v.2.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_v.3.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_v.4.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_v.5.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_v.6.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_v.7.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_v.8.weight 49152\n",
      "backbone.blocks.8.attn.lora_A_v.9.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_v.0.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_v.1.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_v.2.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_v.3.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_v.4.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_v.5.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_v.6.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_v.7.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_v.8.weight 49152\n",
      "backbone.blocks.8.attn.lora_B_v.9.weight 49152\n",
      "backbone.blocks.8.attn.coef_k.0 3000\n",
      "backbone.blocks.8.attn.coef_k.1 3000\n",
      "backbone.blocks.8.attn.coef_k.2 3000\n",
      "backbone.blocks.8.attn.coef_k.3 3000\n",
      "backbone.blocks.8.attn.coef_k.4 3000\n",
      "backbone.blocks.8.attn.coef_k.5 3000\n",
      "backbone.blocks.8.attn.coef_k.6 3000\n",
      "backbone.blocks.8.attn.coef_k.7 3000\n",
      "backbone.blocks.8.attn.coef_k.8 3000\n",
      "backbone.blocks.8.attn.coef_k.9 3000\n",
      "backbone.blocks.8.attn.coef_v.0 3000\n",
      "backbone.blocks.8.attn.coef_v.1 3000\n",
      "backbone.blocks.8.attn.coef_v.2 3000\n",
      "backbone.blocks.8.attn.coef_v.3 3000\n",
      "backbone.blocks.8.attn.coef_v.4 3000\n",
      "backbone.blocks.8.attn.coef_v.5 3000\n",
      "backbone.blocks.8.attn.coef_v.6 3000\n",
      "backbone.blocks.8.attn.coef_v.7 3000\n",
      "backbone.blocks.8.attn.coef_v.8 3000\n",
      "backbone.blocks.8.attn.coef_v.9 3000\n",
      "backbone.blocks.9.attn.lora_A_k.0.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_k.1.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_k.2.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_k.3.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_k.4.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_k.5.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_k.6.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_k.7.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_k.8.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_k.9.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_k.0.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_k.1.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_k.2.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_k.3.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_k.4.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_k.5.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_k.6.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_k.7.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_k.8.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_k.9.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_v.0.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_v.1.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_v.2.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_v.3.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_v.4.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_v.5.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_v.6.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_v.7.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_v.8.weight 49152\n",
      "backbone.blocks.9.attn.lora_A_v.9.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_v.0.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_v.1.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_v.2.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_v.3.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_v.4.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_v.5.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_v.6.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_v.7.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_v.8.weight 49152\n",
      "backbone.blocks.9.attn.lora_B_v.9.weight 49152\n",
      "backbone.blocks.9.attn.coef_k.0 3000\n",
      "backbone.blocks.9.attn.coef_k.1 3000\n",
      "backbone.blocks.9.attn.coef_k.2 3000\n",
      "backbone.blocks.9.attn.coef_k.3 3000\n",
      "backbone.blocks.9.attn.coef_k.4 3000\n",
      "backbone.blocks.9.attn.coef_k.5 3000\n",
      "backbone.blocks.9.attn.coef_k.6 3000\n",
      "backbone.blocks.9.attn.coef_k.7 3000\n",
      "backbone.blocks.9.attn.coef_k.8 3000\n",
      "backbone.blocks.9.attn.coef_k.9 3000\n",
      "backbone.blocks.9.attn.coef_v.0 3000\n",
      "backbone.blocks.9.attn.coef_v.1 3000\n",
      "backbone.blocks.9.attn.coef_v.2 3000\n",
      "backbone.blocks.9.attn.coef_v.3 3000\n",
      "backbone.blocks.9.attn.coef_v.4 3000\n",
      "backbone.blocks.9.attn.coef_v.5 3000\n",
      "backbone.blocks.9.attn.coef_v.6 3000\n",
      "backbone.blocks.9.attn.coef_v.7 3000\n",
      "backbone.blocks.9.attn.coef_v.8 3000\n",
      "backbone.blocks.9.attn.coef_v.9 3000\n",
      "backbone.blocks.10.attn.lora_A_k.0.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_k.1.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_k.2.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_k.3.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_k.4.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_k.5.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_k.6.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_k.7.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_k.8.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_k.9.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_k.0.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_k.1.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_k.2.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_k.3.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_k.4.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_k.5.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_k.6.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_k.7.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_k.8.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_k.9.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_v.0.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_v.1.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_v.2.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_v.3.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_v.4.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_v.5.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_v.6.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_v.7.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_v.8.weight 49152\n",
      "backbone.blocks.10.attn.lora_A_v.9.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_v.0.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_v.1.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_v.2.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_v.3.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_v.4.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_v.5.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_v.6.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_v.7.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_v.8.weight 49152\n",
      "backbone.blocks.10.attn.lora_B_v.9.weight 49152\n",
      "backbone.blocks.10.attn.coef_k.0 3000\n",
      "backbone.blocks.10.attn.coef_k.1 3000\n",
      "backbone.blocks.10.attn.coef_k.2 3000\n",
      "backbone.blocks.10.attn.coef_k.3 3000\n",
      "backbone.blocks.10.attn.coef_k.4 3000\n",
      "backbone.blocks.10.attn.coef_k.5 3000\n",
      "backbone.blocks.10.attn.coef_k.6 3000\n",
      "backbone.blocks.10.attn.coef_k.7 3000\n",
      "backbone.blocks.10.attn.coef_k.8 3000\n",
      "backbone.blocks.10.attn.coef_k.9 3000\n",
      "backbone.blocks.10.attn.coef_v.0 3000\n",
      "backbone.blocks.10.attn.coef_v.1 3000\n",
      "backbone.blocks.10.attn.coef_v.2 3000\n",
      "backbone.blocks.10.attn.coef_v.3 3000\n",
      "backbone.blocks.10.attn.coef_v.4 3000\n",
      "backbone.blocks.10.attn.coef_v.5 3000\n",
      "backbone.blocks.10.attn.coef_v.6 3000\n",
      "backbone.blocks.10.attn.coef_v.7 3000\n",
      "backbone.blocks.10.attn.coef_v.8 3000\n",
      "backbone.blocks.10.attn.coef_v.9 3000\n",
      "backbone.blocks.11.attn.lora_A_k.0.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_k.1.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_k.2.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_k.3.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_k.4.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_k.5.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_k.6.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_k.7.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_k.8.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_k.9.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_k.0.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_k.1.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_k.2.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_k.3.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_k.4.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_k.5.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_k.6.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_k.7.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_k.8.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_k.9.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_v.0.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_v.1.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_v.2.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_v.3.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_v.4.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_v.5.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_v.6.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_v.7.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_v.8.weight 49152\n",
      "backbone.blocks.11.attn.lora_A_v.9.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_v.0.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_v.1.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_v.2.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_v.3.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_v.4.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_v.5.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_v.6.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_v.7.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_v.8.weight 49152\n",
      "backbone.blocks.11.attn.lora_B_v.9.weight 49152\n",
      "backbone.blocks.11.attn.coef_k.0 3000\n",
      "backbone.blocks.11.attn.coef_k.1 3000\n",
      "backbone.blocks.11.attn.coef_k.2 3000\n",
      "backbone.blocks.11.attn.coef_k.3 3000\n",
      "backbone.blocks.11.attn.coef_k.4 3000\n",
      "backbone.blocks.11.attn.coef_k.5 3000\n",
      "backbone.blocks.11.attn.coef_k.6 3000\n",
      "backbone.blocks.11.attn.coef_k.7 3000\n",
      "backbone.blocks.11.attn.coef_k.8 3000\n",
      "backbone.blocks.11.attn.coef_k.9 3000\n",
      "backbone.blocks.11.attn.coef_v.0 3000\n",
      "backbone.blocks.11.attn.coef_v.1 3000\n",
      "backbone.blocks.11.attn.coef_v.2 3000\n",
      "backbone.blocks.11.attn.coef_v.3 3000\n",
      "backbone.blocks.11.attn.coef_v.4 3000\n",
      "backbone.blocks.11.attn.coef_v.5 3000\n",
      "backbone.blocks.11.attn.coef_v.6 3000\n",
      "backbone.blocks.11.attn.coef_v.7 3000\n",
      "backbone.blocks.11.attn.coef_v.8 3000\n",
      "backbone.blocks.11.attn.coef_v.9 3000\n",
      "fc.weight 7680\n",
      "fc.sigma 1\n",
      "Initial training for the first task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [02:19<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0, Epoch 1/1 => Loss 2.286, Train_accy 20.56, Test_accy 36.80\n",
      "This is for the BaseNet initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After BaseNet initialization.\n",
      "Clear the backbone in MultiBranchCosineIncrementalNet, since we are using self.backbones with dual branches\n",
      "Constructed dual branch network.\n",
      "New network structure:\n",
      "MultiBranchCosineIncrementalNet_adapt_AC(\n",
      "  (backbone): Identity()\n",
      "  (backbones): ModuleList(\n",
      "    (0): VisionTransformerBiLoRA(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "        (norm): Identity()\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (blocks): Sequential(\n",
      "        (0): BlockBiLoRA(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention_FFT(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (lora_A_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (lora_A_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (coef_k): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "            (coef_v): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (1): BlockBiLoRA(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention_FFT(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (lora_A_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (lora_A_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (coef_k): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "            (coef_v): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (2): BlockBiLoRA(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention_FFT(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (lora_A_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (lora_A_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (coef_k): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "            (coef_v): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (3): BlockBiLoRA(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention_FFT(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (lora_A_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (lora_A_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (coef_k): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "            (coef_v): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (4): BlockBiLoRA(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention_FFT(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (lora_A_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (lora_A_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (coef_k): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "            (coef_v): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (5): BlockBiLoRA(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention_FFT(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (lora_A_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (lora_A_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (coef_k): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "            (coef_v): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (6): BlockBiLoRA(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention_FFT(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (lora_A_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (lora_A_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (coef_k): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "            (coef_v): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (7): BlockBiLoRA(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention_FFT(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (lora_A_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (lora_A_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (coef_k): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "            (coef_v): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (8): BlockBiLoRA(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention_FFT(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (lora_A_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (lora_A_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (coef_k): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "            (coef_v): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (9): BlockBiLoRA(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention_FFT(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (lora_A_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (lora_A_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (coef_k): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "            (coef_v): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (10): BlockBiLoRA(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention_FFT(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (lora_A_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (lora_A_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (coef_k): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "            (coef_v): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "        (11): BlockBiLoRA(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention_FFT(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            (lora_A_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_k): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (lora_A_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=768, out_features=64, bias=False)\n",
      "            )\n",
      "            (lora_B_v): ModuleList(\n",
      "              (0-9): 10 x Linear(in_features=64, out_features=768, bias=False)\n",
      "            )\n",
      "            (coef_k): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "            (coef_v): ParameterList(\n",
      "                (0): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (3): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (4): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (5): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (6): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (7): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (8): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "                (9): Parameter containing: [torch.float32 of size 3000 (cuda:0)]\n",
      "            )\n",
      "          )\n",
      "          (ls1): Identity()\n",
      "          (drop_path1): Identity()\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (ls2): Identity()\n",
      "          (drop_path2): Identity()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (pre_logits): Identity()\n",
      "      (fc_norm): Identity()\n",
      "      (head): Identity()\n",
      "    )\n",
      "  )\n",
      "  (fc): CosineLinear2()\n",
      ")\n",
      "Use AC model as classifier head.\n",
      "AC model architecture: AC_Linear(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=5000, bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5000, out_features=10, bias=False)\n",
      "  )\n",
      ")\n",
      "Computing class means and covariance matrices...\n",
      "Extracting prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:07<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vectors shape: (500, 768) float32\n",
      "Extracted targets shape: (500,) int64\n",
      "Class 0 covariance matrix shape: (768, 768)\n",
      "Extracting prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:07<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vectors shape: (500, 768) float32\n",
      "Extracted targets shape: (500,) int64\n",
      "Class 1 covariance matrix shape: (768, 768)\n",
      "Extracting prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:07<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vectors shape: (500, 768) float32\n",
      "Extracted targets shape: (500,) int64\n",
      "Class 2 covariance matrix shape: (768, 768)\n",
      "Extracting prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:07<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vectors shape: (500, 768) float32\n",
      "Extracted targets shape: (500,) int64\n",
      "Class 3 covariance matrix shape: (768, 768)\n",
      "Extracting prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:07<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vectors shape: (500, 768) float32\n",
      "Extracted targets shape: (500,) int64\n",
      "Class 4 covariance matrix shape: (768, 768)\n",
      "Extracting prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:07<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vectors shape: (500, 768) float32\n",
      "Extracted targets shape: (500,) int64\n",
      "Class 5 covariance matrix shape: (768, 768)\n",
      "Extracting prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:07<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vectors shape: (500, 768) float32\n",
      "Extracted targets shape: (500,) int64\n",
      "Class 6 covariance matrix shape: (768, 768)\n",
      "Extracting prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:07<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vectors shape: (500, 768) float32\n",
      "Extracted targets shape: (500,) int64\n",
      "Class 7 covariance matrix shape: (768, 768)\n",
      "Extracting prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:07<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vectors shape: (500, 768) float32\n",
      "Extracted targets shape: (500,) int64\n",
      "Class 8 covariance matrix shape: (768, 768)\n",
      "Extracting prototypes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:07<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vectors shape: (500, 768) float32\n",
      "Extracted targets shape: (500,) int64\n",
      "Class 9 covariance matrix shape: (768, 768)\n",
      "Starting class alignment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Alignment: 100%|██████████| 53/53 [01:10<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape:  torch.Size([5000, 5000])\n",
      "Label shape torch.Size([5000])\n",
      "One-hot label shape:  torch.Size([5000, 10])\n",
      "Optimising ridge parameter...\n",
      "selected lambda = 1000.0\n",
      "gamma 1000.0\n",
      "numpy inverse\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "`np.mat` was removed in the NumPy 2.0 release. Use `np.asmatrix` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2964465400.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m\"Trainable params: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincremental_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcnn_accy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnme_accy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/Moal/Moal/models/adapt_ac_com_sdc_EMA_auto.py\u001b[0m in \u001b[0;36mincremental_train\u001b[0;34m(self, data_manager)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Multiple GPUs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiple_gpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader_for_protonet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiple_gpus\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/Moal/Moal/models/adapt_ac_com_sdc_EMA_auto.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_loader, test_loader, train_loader_for_protonet)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;31m# AL training process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_means\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/Moal/Moal/models/adapt_ac_com_sdc_EMA_auto.py\u001b[0m in \u001b[0;36mcls_align\u001b[0;34m(self, trainloader, model)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy inverse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_cor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mridge\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mac_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mDelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mR\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mcrs_cor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__expired_attributes__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    398\u001b[0m                 \u001b[0;34mf\"`np.{attr}` was removed in the NumPy 2.0 release. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0;34mf\"{__expired_attributes__[attr]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: `np.mat` was removed in the NumPy 2.0 release. Use `np.asmatrix` instead."
     ]
    }
   ],
   "source": [
    "import logging\n",
    "cnn_curve, nme_curve = {\"top1\": [], \"top5\": []}, {\"top1\": [], \"top5\": []}\n",
    "\n",
    "all_his_acc = []\n",
    "for task in range(data_manager.nb_tasks):\n",
    "    logging.info(\"All params: {}\".format(count_parameters(model._network)))\n",
    "    logging.info(\n",
    "        \"Trainable params: {}\".format(count_parameters(model._network, True))\n",
    "    )\n",
    "    model.incremental_train(data_manager)\n",
    "    cnn_accy, nme_accy = model.eval_task()\n",
    "    model.after_task()\n",
    "\n",
    "    if nme_accy is not None:\n",
    "        logging.info(\"CNN: {}\".format(cnn_accy[\"grouped\"]))\n",
    "        logging.info(\"NME: {}\".format(nme_accy[\"grouped\"]))\n",
    "\n",
    "        all_his_acc.append(cnn_accy[\"grouped\"].values())\n",
    "        cnn_curve[\"top1\"].append(cnn_accy[\"top1\"])\n",
    "        cnn_curve[\"top5\"].append(cnn_accy[\"top5\"])\n",
    "\n",
    "        nme_curve[\"top1\"].append(nme_accy[\"top1\"])\n",
    "        nme_curve[\"top5\"].append(nme_accy[\"top5\"])\n",
    "\n",
    "        logging.info(\"CNN top1 curve: {}\".format(cnn_curve[\"top1\"]))\n",
    "        logging.info(\"CNN top5 curve: {}\".format(cnn_curve[\"top5\"]))\n",
    "        logging.info(\"NME top1 curve: {}\".format(nme_curve[\"top1\"]))\n",
    "        logging.info(\"NME top5 curve: {}\\n\".format(nme_curve[\"top5\"]))\n",
    "\n",
    "        print('Average Accuracy (CNN):', sum(cnn_curve[\"top1\"])/len(cnn_curve[\"top1\"]))\n",
    "        print('Average Accuracy (NME):', sum(nme_curve[\"top1\"])/len(nme_curve[\"top1\"]))\n",
    "\n",
    "        logging.info(\"Average Accuracy (CNN): {}\".format(sum(cnn_curve[\"top1\"])/len(cnn_curve[\"top1\"])))\n",
    "        logging.info(\"Average Accuracy (NME): {}\".format(sum(nme_curve[\"top1\"])/len(nme_curve[\"top1\"])))\n",
    "    else:\n",
    "        logging.info(\"No NME accuracy.\")\n",
    "        logging.info(\"CNN: {}\".format(cnn_accy[\"grouped\"]))\n",
    "\n",
    "        cnn_accy[\"grouped\"].pop('total')\n",
    "        cnn_accy[\"grouped\"].pop('old')\n",
    "        cnn_accy[\"grouped\"].pop('new')\n",
    "        all_his_acc.append(list(cnn_accy[\"grouped\"].values()))\n",
    "\n",
    "        cnn_curve[\"top1\"].append(cnn_accy[\"top1\"])\n",
    "        cnn_curve[\"top5\"].append(cnn_accy[\"top5\"])\n",
    "\n",
    "        logging.info(\"CNN top1 curve: {}\".format(cnn_curve[\"top1\"]))\n",
    "        logging.info(\"CNN top5 curve: {}\\n\".format(cnn_curve[\"top5\"]))\n",
    "\n",
    "        print('Average Accuracy (CNN):', sum(cnn_curve[\"top1\"])/len(cnn_curve[\"top1\"]))\n",
    "        logging.info(\"Average Accuracy (CNN): {} \\n\".format(sum(cnn_curve[\"top1\"])/len(cnn_curve[\"top1\"])))\n",
    "logging.info(\"All History Accuracy (CNN): {} \\n\".format(all_his_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa2eeb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f84c86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 224, 224])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 2 \n",
    "dummy_input = torch.randn(\n",
    "    batch_size,\n",
    "    3, 224, 224,\n",
    "    dtype=torch.float32,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "print(dummy_input.shape)  # (B, 3, 224, 224)\n",
    "print(dummy_input.dtype)  # torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54df58e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.forward(dummy_input, task=0)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee44a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Moal.backbone.vision_transformer_adapter import vit_base_patch16_224_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1a0afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "tuning_config = EasyDict(\n",
    "    # AdaptFormer.\n",
    "    ffn_adapt=True,\n",
    "    ffn_option=\"parallel\",\n",
    "    ffn_adapter_layernorm_option=\"none\",\n",
    "    ffn_adapter_init_option=\"lora\",\n",
    "    ffn_adapter_scalar=\"0.1\",\n",
    "    ffn_num=64,\n",
    "    d_model=768,\n",
    "    # VPT related\n",
    "    vpt_on=False,\n",
    "    vpt_num=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ceee2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm using ViT with adapters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac30ac1b11ab44e8bf380531a0b1742a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['blocks.0.adaptmlp.down_proj.weight', 'blocks.0.adaptmlp.down_proj.bias', 'blocks.0.adaptmlp.up_proj.weight', 'blocks.0.adaptmlp.up_proj.bias', 'blocks.1.adaptmlp.down_proj.weight', 'blocks.1.adaptmlp.down_proj.bias', 'blocks.1.adaptmlp.up_proj.weight', 'blocks.1.adaptmlp.up_proj.bias', 'blocks.2.adaptmlp.down_proj.weight', 'blocks.2.adaptmlp.down_proj.bias', 'blocks.2.adaptmlp.up_proj.weight', 'blocks.2.adaptmlp.up_proj.bias', 'blocks.3.adaptmlp.down_proj.weight', 'blocks.3.adaptmlp.down_proj.bias', 'blocks.3.adaptmlp.up_proj.weight', 'blocks.3.adaptmlp.up_proj.bias', 'blocks.4.adaptmlp.down_proj.weight', 'blocks.4.adaptmlp.down_proj.bias', 'blocks.4.adaptmlp.up_proj.weight', 'blocks.4.adaptmlp.up_proj.bias', 'blocks.5.adaptmlp.down_proj.weight', 'blocks.5.adaptmlp.down_proj.bias', 'blocks.5.adaptmlp.up_proj.weight', 'blocks.5.adaptmlp.up_proj.bias', 'blocks.6.adaptmlp.down_proj.weight', 'blocks.6.adaptmlp.down_proj.bias', 'blocks.6.adaptmlp.up_proj.weight', 'blocks.6.adaptmlp.up_proj.bias', 'blocks.7.adaptmlp.down_proj.weight', 'blocks.7.adaptmlp.down_proj.bias', 'blocks.7.adaptmlp.up_proj.weight', 'blocks.7.adaptmlp.up_proj.bias', 'blocks.8.adaptmlp.down_proj.weight', 'blocks.8.adaptmlp.down_proj.bias', 'blocks.8.adaptmlp.up_proj.weight', 'blocks.8.adaptmlp.up_proj.bias', 'blocks.9.adaptmlp.down_proj.weight', 'blocks.9.adaptmlp.down_proj.bias', 'blocks.9.adaptmlp.up_proj.weight', 'blocks.9.adaptmlp.up_proj.bias', 'blocks.10.adaptmlp.down_proj.weight', 'blocks.10.adaptmlp.down_proj.bias', 'blocks.10.adaptmlp.up_proj.weight', 'blocks.10.adaptmlp.up_proj.bias', 'blocks.11.adaptmlp.down_proj.weight', 'blocks.11.adaptmlp.down_proj.bias', 'blocks.11.adaptmlp.up_proj.weight', 'blocks.11.adaptmlp.up_proj.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "ref_model = vit_base_patch16_224_adapter(num_classes=0,\n",
    "                    global_pool=False, drop_path_rate=0.0, tuning_config=tuning_config)\n",
    "ref_model.out_dim=768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de4df365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_y = ref_model.forward(dummy_input)\n",
    "ref_y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
