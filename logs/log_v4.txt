Initial training for the first task.
100%|██████████| 79/79 [01:01<00:00,  1.28it/s]
Task 0, Epoch 1/60 => Loss 2.087, Train_accy 69.28, Test_accy 95.90
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 2/60 => Loss 1.281, Train_accy 88.34, Test_accy 96.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 3/60 => Loss 0.729, Train_accy 90.40, Test_accy 97.00
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 4/60 => Loss 0.503, Train_accy 90.88, Test_accy 97.60
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 5/60 => Loss 0.391, Train_accy 91.66, Test_accy 97.70
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 6/60 => Loss 0.337, Train_accy 91.60, Test_accy 97.60
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 7/60 => Loss 0.303, Train_accy 91.94, Test_accy 97.90
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 8/60 => Loss 0.283, Train_accy 92.42, Test_accy 97.90
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 9/60 => Loss 0.262, Train_accy 92.66, Test_accy 98.00
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 10/60 => Loss 0.244, Train_accy 92.84, Test_accy 98.10
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 11/60 => Loss 0.237, Train_accy 93.44, Test_accy 98.20
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 12/60 => Loss 0.236, Train_accy 93.12, Test_accy 98.20
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 13/60 => Loss 0.228, Train_accy 93.32, Test_accy 98.10
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 14/60 => Loss 0.215, Train_accy 93.56, Test_accy 98.40
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 15/60 => Loss 0.204, Train_accy 93.88, Test_accy 98.50
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 16/60 => Loss 0.212, Train_accy 93.68, Test_accy 98.60
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 17/60 => Loss 0.207, Train_accy 93.76, Test_accy 98.50
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 0, Epoch 18/60 => Loss 0.193, Train_accy 94.16, Test_accy 98.40
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 19/60 => Loss 0.203, Train_accy 93.54, Test_accy 98.40
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 0, Epoch 20/60 => Loss 0.208, Train_accy 93.92, Test_accy 98.50
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 21/60 => Loss 0.202, Train_accy 93.82, Test_accy 98.50
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 22/60 => Loss 0.199, Train_accy 93.90, Test_accy 98.60
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 23/60 => Loss 0.196, Train_accy 94.12, Test_accy 98.40
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 24/60 => Loss 0.187, Train_accy 94.08, Test_accy 98.40
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 0, Epoch 25/60 => Loss 0.196, Train_accy 93.96, Test_accy 98.40
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 26/60 => Loss 0.196, Train_accy 93.78, Test_accy 98.70
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 27/60 => Loss 0.179, Train_accy 94.48, Test_accy 98.60
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 28/60 => Loss 0.187, Train_accy 94.16, Test_accy 98.60
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 29/60 => Loss 0.183, Train_accy 94.16, Test_accy 98.60
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 30/60 => Loss 0.188, Train_accy 93.84, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 31/60 => Loss 0.177, Train_accy 94.26, Test_accy 98.70
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 32/60 => Loss 0.190, Train_accy 94.04, Test_accy 98.90
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 33/60 => Loss 0.186, Train_accy 93.94, Test_accy 98.60
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 34/60 => Loss 0.171, Train_accy 94.58, Test_accy 98.70
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 35/60 => Loss 0.186, Train_accy 94.40, Test_accy 98.70
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 36/60 => Loss 0.177, Train_accy 94.22, Test_accy 98.70
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 37/60 => Loss 0.184, Train_accy 94.26, Test_accy 98.70
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 38/60 => Loss 0.187, Train_accy 93.68, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 39/60 => Loss 0.163, Train_accy 94.80, Test_accy 98.60
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 40/60 => Loss 0.172, Train_accy 94.62, Test_accy 98.70
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 41/60 => Loss 0.175, Train_accy 94.20, Test_accy 98.90
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 42/60 => Loss 0.168, Train_accy 94.24, Test_accy 98.90
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 43/60 => Loss 0.161, Train_accy 94.76, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 44/60 => Loss 0.163, Train_accy 94.92, Test_accy 98.70
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 45/60 => Loss 0.189, Train_accy 94.06, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 46/60 => Loss 0.174, Train_accy 94.80, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 47/60 => Loss 0.181, Train_accy 93.82, Test_accy 98.70
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 48/60 => Loss 0.174, Train_accy 94.60, Test_accy 98.70
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 49/60 => Loss 0.188, Train_accy 93.74, Test_accy 98.60
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 50/60 => Loss 0.170, Train_accy 94.66, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 51/60 => Loss 0.177, Train_accy 94.06, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 52/60 => Loss 0.189, Train_accy 93.90, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 53/60 => Loss 0.185, Train_accy 94.02, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 54/60 => Loss 0.162, Train_accy 94.80, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 55/60 => Loss 0.173, Train_accy 94.88, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 56/60 => Loss 0.168, Train_accy 94.56, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 57/60 => Loss 0.167, Train_accy 94.64, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 58/60 => Loss 0.161, Train_accy 94.90, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 59/60 => Loss 0.181, Train_accy 94.18, Test_accy 98.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 0, Epoch 60/60 => Loss 0.157, Train_accy 94.76, Test_accy 98.80
This is for the BaseNet initialization.
I'm using ViT with adapters.
_IncompatibleKeys(missing_keys=['blocks.0.adaptmlp.down_proj.weight', 'blocks.0.adaptmlp.down_proj.bias', 'blocks.0.adaptmlp.up_proj.weight', 'blocks.0.adaptmlp.up_proj.bias', 'blocks.1.adaptmlp.down_proj.weight', 'blocks.1.adaptmlp.down_proj.bias', 'blocks.1.adaptmlp.up_proj.weight', 'blocks.1.adaptmlp.up_proj.bias', 'blocks.2.adaptmlp.down_proj.weight', 'blocks.2.adaptmlp.down_proj.bias', 'blocks.2.adaptmlp.up_proj.weight', 'blocks.2.adaptmlp.up_proj.bias', 'blocks.3.adaptmlp.down_proj.weight', 'blocks.3.adaptmlp.down_proj.bias', 'blocks.3.adaptmlp.up_proj.weight', 'blocks.3.adaptmlp.up_proj.bias', 'blocks.4.adaptmlp.down_proj.weight', 'blocks.4.adaptmlp.down_proj.bias', 'blocks.4.adaptmlp.up_proj.weight', 'blocks.4.adaptmlp.up_proj.bias', 'blocks.5.adaptmlp.down_proj.weight', 'blocks.5.adaptmlp.down_proj.bias', 'blocks.5.adaptmlp.up_proj.weight', 'blocks.5.adaptmlp.up_proj.bias', 'blocks.6.adaptmlp.down_proj.weight', 'blocks.6.adaptmlp.down_proj.bias', 'blocks.6.adaptmlp.up_proj.weight', 'blocks.6.adaptmlp.up_proj.bias', 'blocks.7.adaptmlp.down_proj.weight', 'blocks.7.adaptmlp.down_proj.bias', 'blocks.7.adaptmlp.up_proj.weight', 'blocks.7.adaptmlp.up_proj.bias', 'blocks.8.adaptmlp.down_proj.weight', 'blocks.8.adaptmlp.down_proj.bias', 'blocks.8.adaptmlp.up_proj.weight', 'blocks.8.adaptmlp.up_proj.bias', 'blocks.9.adaptmlp.down_proj.weight', 'blocks.9.adaptmlp.down_proj.bias', 'blocks.9.adaptmlp.up_proj.weight', 'blocks.9.adaptmlp.up_proj.bias', 'blocks.10.adaptmlp.down_proj.weight', 'blocks.10.adaptmlp.down_proj.bias', 'blocks.10.adaptmlp.up_proj.weight', 'blocks.10.adaptmlp.up_proj.bias', 'blocks.11.adaptmlp.down_proj.weight', 'blocks.11.adaptmlp.down_proj.bias', 'blocks.11.adaptmlp.up_proj.weight', 'blocks.11.adaptmlp.up_proj.bias'], unexpected_keys=[])
After BaseNet initialization.
Clear the backbone in MultiBranchCosineIncrementalNet, since we are using self.backbones with dual branches
Constructed dual branch network.
New network structure:
MultiBranchCosineIncrementalNet_adapt_AC(
  (backbone): Identity()
  (backbones): ModuleList(
    (0): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (mlp_drop): Dropout(p=0.0, inplace=False)
          (adaptmlp): Adapter(
            (down_proj): Linear(in_features=768, out_features=64, bias=True)
            (non_linear_func): ReLU()
            (up_proj): Linear(in_features=64, out_features=768, bias=True)
          )
        )
        (1): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (mlp_drop): Dropout(p=0.0, inplace=False)
          (adaptmlp): Adapter(
            (down_proj): Linear(in_features=768, out_features=64, bias=True)
            (non_linear_func): ReLU()
            (up_proj): Linear(in_features=64, out_features=768, bias=True)
          )
        )
        (2): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (mlp_drop): Dropout(p=0.0, inplace=False)
          (adaptmlp): Adapter(
            (down_proj): Linear(in_features=768, out_features=64, bias=True)
            (non_linear_func): ReLU()
            (up_proj): Linear(in_features=64, out_features=768, bias=True)
          )
        )
        (3): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (mlp_drop): Dropout(p=0.0, inplace=False)
          (adaptmlp): Adapter(
            (down_proj): Linear(in_features=768, out_features=64, bias=True)
            (non_linear_func): ReLU()
            (up_proj): Linear(in_features=64, out_features=768, bias=True)
          )
        )
        (4): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (mlp_drop): Dropout(p=0.0, inplace=False)
          (adaptmlp): Adapter(
            (down_proj): Linear(in_features=768, out_features=64, bias=True)
            (non_linear_func): ReLU()
            (up_proj): Linear(in_features=64, out_features=768, bias=True)
          )
        )
        (5): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (mlp_drop): Dropout(p=0.0, inplace=False)
          (adaptmlp): Adapter(
            (down_proj): Linear(in_features=768, out_features=64, bias=True)
            (non_linear_func): ReLU()
            (up_proj): Linear(in_features=64, out_features=768, bias=True)
          )
        )
        (6): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (mlp_drop): Dropout(p=0.0, inplace=False)
          (adaptmlp): Adapter(
            (down_proj): Linear(in_features=768, out_features=64, bias=True)
            (non_linear_func): ReLU()
            (up_proj): Linear(in_features=64, out_features=768, bias=True)
          )
        )
        (7): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (mlp_drop): Dropout(p=0.0, inplace=False)
          (adaptmlp): Adapter(
            (down_proj): Linear(in_features=768, out_features=64, bias=True)
            (non_linear_func): ReLU()
            (up_proj): Linear(in_features=64, out_features=768, bias=True)
          )
        )
        (8): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (mlp_drop): Dropout(p=0.0, inplace=False)
          (adaptmlp): Adapter(
            (down_proj): Linear(in_features=768, out_features=64, bias=True)
            (non_linear_func): ReLU()
            (up_proj): Linear(in_features=64, out_features=768, bias=True)
          )
        )
        (9): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (mlp_drop): Dropout(p=0.0, inplace=False)
          (adaptmlp): Adapter(
            (down_proj): Linear(in_features=768, out_features=64, bias=True)
            (non_linear_func): ReLU()
            (up_proj): Linear(in_features=64, out_features=768, bias=True)
          )
        )
        (10): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (mlp_drop): Dropout(p=0.0, inplace=False)
          (adaptmlp): Adapter(
            (down_proj): Linear(in_features=768, out_features=64, bias=True)
            (non_linear_func): ReLU()
            (up_proj): Linear(in_features=64, out_features=768, bias=True)
          )
        )
        (11): Block(
          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (q_proj): Linear(in_features=768, out_features=768, bias=True)
            (v_proj): Linear(in_features=768, out_features=768, bias=True)
            (k_proj): Linear(in_features=768, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (mlp_drop): Dropout(p=0.0, inplace=False)
          (adaptmlp): Adapter(
            (down_proj): Linear(in_features=768, out_features=64, bias=True)
            (non_linear_func): ReLU()
            (up_proj): Linear(in_features=64, out_features=768, bias=True)
          )
        )
      )
      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (pre_logits): Identity()
      (head): Identity()
    )
  )
  (fc): CosineLinear2()
)
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=10, bias=False)
  )
)
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 0 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 1 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.26it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 2 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 3 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.26it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 4 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.25it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 5 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.25it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 6 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.25it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 7 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.25it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 8 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.25it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 9 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Starting class alignment...
Alignment: 100%|██████████| 79/79 [00:32<00:00,  2.47batch/s]
Embedding shape:  torch.Size([5000, 5000])
Label shape torch.Size([5000])
One-hot label shape:  torch.Size([5000, 10])
Optimising ridge parameter...
selected lambda = 100.0
gamma 100.0
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 0 Elapsed time:  4158.312291622162
Average Accuracy (CNN): 98.6
Learning on 10-20
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([10, 768])
New weight (Cosine FC) torch.Size([20, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=20, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([10, 5000])
New weight (AC model) torch.Size([20, 5000])
90,943,649 total parameters.
1,204,993 training parameters.
Progressive training for task 1
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 1/40 => Loss 1.720, Train_accy 86.44, Test_accy 47.20
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 2/40 => Loss 0.653, Train_accy 89.48, Test_accy 47.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 3/40 => Loss 0.434, Train_accy 90.94, Test_accy 47.90
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 4/40 => Loss 0.359, Train_accy 91.24, Test_accy 48.25
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 5/40 => Loss 0.329, Train_accy 91.94, Test_accy 48.50
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 6/40 => Loss 0.309, Train_accy 92.02, Test_accy 48.35
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 7/40 => Loss 0.289, Train_accy 92.18, Test_accy 48.35
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 8/40 => Loss 0.260, Train_accy 92.72, Test_accy 48.55
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 9/40 => Loss 0.260, Train_accy 92.62, Test_accy 48.45
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 10/40 => Loss 0.254, Train_accy 92.72, Test_accy 48.35
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 1, Epoch 11/40 => Loss 0.261, Train_accy 92.74, Test_accy 48.30
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 1, Epoch 12/40 => Loss 0.243, Train_accy 93.04, Test_accy 48.70
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 13/40 => Loss 0.230, Train_accy 93.66, Test_accy 48.55
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 14/40 => Loss 0.236, Train_accy 93.32, Test_accy 48.25
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 1, Epoch 15/40 => Loss 0.253, Train_accy 92.40, Test_accy 48.65
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 1, Epoch 16/40 => Loss 0.217, Train_accy 93.86, Test_accy 48.80
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 1, Epoch 17/40 => Loss 0.224, Train_accy 93.54, Test_accy 48.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 18/40 => Loss 0.210, Train_accy 93.92, Test_accy 48.55
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 19/40 => Loss 0.221, Train_accy 93.46, Test_accy 48.80
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 1, Epoch 20/40 => Loss 0.226, Train_accy 93.38, Test_accy 48.65
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 21/40 => Loss 0.223, Train_accy 93.30, Test_accy 48.65
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 22/40 => Loss 0.228, Train_accy 93.04, Test_accy 48.80
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 1, Epoch 23/40 => Loss 0.219, Train_accy 93.74, Test_accy 48.65
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 1, Epoch 24/40 => Loss 0.221, Train_accy 93.42, Test_accy 48.70
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 25/40 => Loss 0.214, Train_accy 93.52, Test_accy 48.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 26/40 => Loss 0.226, Train_accy 92.96, Test_accy 48.80
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 1, Epoch 27/40 => Loss 0.202, Train_accy 93.94, Test_accy 48.75
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 28/40 => Loss 0.206, Train_accy 93.96, Test_accy 48.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 29/40 => Loss 0.204, Train_accy 94.10, Test_accy 48.65
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 1, Epoch 30/40 => Loss 0.203, Train_accy 94.18, Test_accy 48.85
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 1, Epoch 31/40 => Loss 0.216, Train_accy 93.48, Test_accy 48.75
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 1, Epoch 32/40 => Loss 0.209, Train_accy 93.56, Test_accy 48.80
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 1, Epoch 33/40 => Loss 0.195, Train_accy 94.34, Test_accy 48.90
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 34/40 => Loss 0.203, Train_accy 94.34, Test_accy 48.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 35/40 => Loss 0.216, Train_accy 93.62, Test_accy 48.85
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 36/40 => Loss 0.207, Train_accy 93.84, Test_accy 48.85
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 37/40 => Loss 0.195, Train_accy 94.08, Test_accy 48.90
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 38/40 => Loss 0.206, Train_accy 93.54, Test_accy 48.90
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 1, Epoch 39/40 => Loss 0.210, Train_accy 93.92, Test_accy 48.90
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 1, Epoch 40/40 => Loss 0.196, Train_accy 94.16, Test_accy 48.90
Cache size: 79
Task 1, Epoch 40/40 => Loss 0.196, Train_accy 94.16, Test_accy 48.90
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.16it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 10 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 11 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 12 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 13 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 14 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 15 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.21it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 16 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 17 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 18 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 19 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  0%|          | 0/50 [00:00<?, ?it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.40it/s]
best_loss: 0.09443449437618255
Computing class relations...
Old means shape: (10, 768)
New means shape: (10, 768)
Class relations: [17 12 15 10 11 13 11 14 16 17]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 10000
Feature dataset dimension: 768
Label dataset size: 10000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 20])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 157/157 [00:05<00:00, 27.54batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 1 Elapsed time:  3070.1646728515625
Average Accuracy (CNN): 96.4
Learning on 20-30
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([20, 768])
New weight (Cosine FC) torch.Size([30, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=30, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([20, 5000])
New weight (AC model) torch.Size([30, 5000])
91,001,329 total parameters.
1,212,673 training parameters.
Progressive training for task 2
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 1/40 => Loss 0.761, Train_accy 82.68, Test_accy 58.67
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 2/40 => Loss 0.527, Train_accy 88.42, Test_accy 57.70
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 3/40 => Loss 0.477, Train_accy 88.76, Test_accy 56.60
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 4/40 => Loss 0.432, Train_accy 89.50, Test_accy 55.87
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 5/40 => Loss 0.399, Train_accy 90.46, Test_accy 55.07
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 6/40 => Loss 0.389, Train_accy 90.34, Test_accy 54.77
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 7/40 => Loss 0.378, Train_accy 90.20, Test_accy 54.70
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 8/40 => Loss 0.381, Train_accy 90.34, Test_accy 54.07
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 9/40 => Loss 0.333, Train_accy 91.48, Test_accy 54.23
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 10/40 => Loss 0.349, Train_accy 90.46, Test_accy 54.10
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 11/40 => Loss 0.324, Train_accy 91.56, Test_accy 52.77
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 12/40 => Loss 0.321, Train_accy 91.32, Test_accy 53.13
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 13/40 => Loss 0.330, Train_accy 91.64, Test_accy 53.33
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 14/40 => Loss 0.317, Train_accy 91.42, Test_accy 52.50
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 15/40 => Loss 0.304, Train_accy 91.90, Test_accy 52.73
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 16/40 => Loss 0.302, Train_accy 91.78, Test_accy 51.97
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 17/40 => Loss 0.310, Train_accy 91.68, Test_accy 52.57
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 18/40 => Loss 0.300, Train_accy 91.72, Test_accy 52.37
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 19/40 => Loss 0.303, Train_accy 91.84, Test_accy 52.37
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 20/40 => Loss 0.283, Train_accy 92.34, Test_accy 52.73
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 21/40 => Loss 0.298, Train_accy 91.72, Test_accy 52.13
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 22/40 => Loss 0.296, Train_accy 92.36, Test_accy 51.67
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 23/40 => Loss 0.276, Train_accy 92.54, Test_accy 51.67
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 24/40 => Loss 0.305, Train_accy 91.54, Test_accy 51.73
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 25/40 => Loss 0.275, Train_accy 92.40, Test_accy 51.97
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 26/40 => Loss 0.271, Train_accy 93.02, Test_accy 52.17
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 27/40 => Loss 0.287, Train_accy 92.28, Test_accy 52.10
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 28/40 => Loss 0.280, Train_accy 92.56, Test_accy 52.07
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 29/40 => Loss 0.269, Train_accy 93.00, Test_accy 52.00
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 30/40 => Loss 0.294, Train_accy 91.76, Test_accy 51.83
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 31/40 => Loss 0.288, Train_accy 92.38, Test_accy 51.67
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 32/40 => Loss 0.280, Train_accy 92.30, Test_accy 51.13
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 33/40 => Loss 0.277, Train_accy 92.38, Test_accy 51.47
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 34/40 => Loss 0.278, Train_accy 92.32, Test_accy 51.47
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 35/40 => Loss 0.268, Train_accy 92.54, Test_accy 51.53
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 36/40 => Loss 0.265, Train_accy 92.48, Test_accy 51.67
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 37/40 => Loss 0.275, Train_accy 92.56, Test_accy 51.53
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 2, Epoch 38/40 => Loss 0.251, Train_accy 93.14, Test_accy 51.60
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 39/40 => Loss 0.273, Train_accy 92.32, Test_accy 51.60
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 2, Epoch 40/40 => Loss 0.272, Train_accy 92.30, Test_accy 51.60
Cache size: 79
Task 2, Epoch 40/40 => Loss 0.272, Train_accy 92.30, Test_accy 51.60
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 20 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 21 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 22 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 23 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.21it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 24 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 25 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 26 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 27 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 28 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 29 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  0%|          | 0/50 [00:00<?, ?it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.40it/s]
best_loss: 0.121037473320961
Computing class relations...
Old means shape: (20, 768)
New means shape: (10, 768)
Class relations: [25 24 28 28 23 22 25 23 20 25 25 21 24 22 21 28 20 24 27 28]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 15000
Feature dataset dimension: 768
Label dataset size: 15000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 30])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 235/235 [00:08<00:00, 27.18batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 2 Elapsed time:  3337.013170480728
Average Accuracy (CNN): 94.97666666666667
Learning on 30-40
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([30, 768])
New weight (Cosine FC) torch.Size([40, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=40, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([30, 5000])
New weight (AC model) torch.Size([40, 5000])
91,059,009 total parameters.
1,220,353 training parameters.
Progressive training for task 3
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 1/40 => Loss 0.617, Train_accy 84.82, Test_accy 57.65
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 2/40 => Loss 0.384, Train_accy 90.52, Test_accy 56.20
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 3, Epoch 3/40 => Loss 0.341, Train_accy 91.74, Test_accy 54.38
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 4/40 => Loss 0.322, Train_accy 92.20, Test_accy 53.62
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 3, Epoch 5/40 => Loss 0.315, Train_accy 92.32, Test_accy 52.40
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 6/40 => Loss 0.295, Train_accy 92.32, Test_accy 52.70
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 7/40 => Loss 0.302, Train_accy 92.16, Test_accy 51.60
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 8/40 => Loss 0.291, Train_accy 92.40, Test_accy 51.48
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 3, Epoch 9/40 => Loss 0.271, Train_accy 93.06, Test_accy 50.90
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 10/40 => Loss 0.276, Train_accy 92.54, Test_accy 50.82
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 11/40 => Loss 0.265, Train_accy 92.98, Test_accy 51.12
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 3, Epoch 12/40 => Loss 0.277, Train_accy 92.62, Test_accy 50.58
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 3, Epoch 13/40 => Loss 0.257, Train_accy 93.02, Test_accy 49.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 3, Epoch 14/40 => Loss 0.263, Train_accy 92.88, Test_accy 49.28
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 3, Epoch 15/40 => Loss 0.249, Train_accy 93.40, Test_accy 50.12
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 16/40 => Loss 0.247, Train_accy 93.28, Test_accy 50.85
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 3, Epoch 17/40 => Loss 0.238, Train_accy 93.40, Test_accy 49.25
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 3, Epoch 18/40 => Loss 0.228, Train_accy 93.86, Test_accy 49.30
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 19/40 => Loss 0.231, Train_accy 93.72, Test_accy 49.32
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 20/40 => Loss 0.245, Train_accy 93.14, Test_accy 48.48
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 21/40 => Loss 0.242, Train_accy 93.34, Test_accy 48.92
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 22/40 => Loss 0.226, Train_accy 93.90, Test_accy 48.68

100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 23/40 => Loss 0.219, Train_accy 94.04, Test_accy 48.30
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 24/40 => Loss 0.247, Train_accy 93.12, Test_accy 48.28
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 25/40 => Loss 0.228, Train_accy 93.84, Test_accy 48.00
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 3, Epoch 26/40 => Loss 0.229, Train_accy 93.36, Test_accy 47.70
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 27/40 => Loss 0.209, Train_accy 94.02, Test_accy 47.70
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 28/40 => Loss 0.211, Train_accy 94.04, Test_accy 47.60
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 3, Epoch 29/40 => Loss 0.225, Train_accy 93.64, Test_accy 47.35
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 30/40 => Loss 0.218, Train_accy 94.00, Test_accy 47.38
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 3, Epoch 31/40 => Loss 0.218, Train_accy 93.92, Test_accy 46.85
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 32/40 => Loss 0.210, Train_accy 93.96, Test_accy 47.65
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 33/40 => Loss 0.212, Train_accy 93.86, Test_accy 47.62
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 3, Epoch 34/40 => Loss 0.216, Train_accy 94.04, Test_accy 46.95
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 35/40 => Loss 0.221, Train_accy 93.98, Test_accy 47.08
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 36/40 => Loss 0.225, Train_accy 93.66, Test_accy 46.98
100%|██████████| 79/79 [01:01<00:00,  1.28it/s]
Task 3, Epoch 37/40 => Loss 0.219, Train_accy 94.32, Test_accy 47.02
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 38/40 => Loss 0.243, Train_accy 93.40, Test_accy 46.92
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 3, Epoch 39/40 => Loss 0.230, Train_accy 93.46, Test_accy 46.90
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 3, Epoch 40/40 => Loss 0.223, Train_accy 93.66, Test_accy 46.90
Cache size: 79
Task 3, Epoch 40/40 => Loss 0.223, Train_accy 93.66, Test_accy 46.90
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.17it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 30 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 31 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 32 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 33 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 34 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 35 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 36 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 37 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 38 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 39 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  2%|▏         | 1/50 [00:00<00:09,  5.21it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.45it/s]
best_loss: 0.07712168473005294
Computing class relations...
Old means shape: (30, 768)
New means shape: (10, 768)
Class relations: [37 39 32 30 39 32 32 37 37 37 35 39 32 39 39 32 37 37 39 39 37 36 39 39
 36 34 34 39 32 32]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 20000
Feature dataset dimension: 768
Label dataset size: 20000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 40])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 313/313 [00:11<00:00, 27.13batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 3 Elapsed time:  3597.826252937317
Average Accuracy (CNN): 93.6275
Learning on 40-50
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([40, 768])
New weight (Cosine FC) torch.Size([50, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=50, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([40, 5000])
New weight (AC model) torch.Size([50, 5000])
91,116,689 total parameters.
1,228,033 training parameters.
Progressive training for task 4
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 1/40 => Loss 0.628, Train_accy 84.66, Test_accy 60.34
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 2/40 => Loss 0.402, Train_accy 88.82, Test_accy 59.74
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 3/40 => Loss 0.360, Train_accy 90.40, Test_accy 56.60
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 4/40 => Loss 0.349, Train_accy 90.00, Test_accy 56.72
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 5/40 => Loss 0.325, Train_accy 91.38, Test_accy 57.48
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 6/40 => Loss 0.312, Train_accy 91.80, Test_accy 56.04
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 7/40 => Loss 0.309, Train_accy 91.74, Test_accy 54.88
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 8/40 => Loss 0.313, Train_accy 91.46, Test_accy 55.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 9/40 => Loss 0.299, Train_accy 91.84, Test_accy 55.44
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 10/40 => Loss 0.305, Train_accy 91.48, Test_accy 54.66
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 11/40 => Loss 0.290, Train_accy 92.06, Test_accy 54.68
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 12/40 => Loss 0.292, Train_accy 91.94, Test_accy 54.36
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 13/40 => Loss 0.278, Train_accy 92.78, Test_accy 54.24
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 14/40 => Loss 0.275, Train_accy 92.54, Test_accy 54.82
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 15/40 => Loss 0.279, Train_accy 92.50, Test_accy 54.24
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 16/40 => Loss 0.264, Train_accy 92.82, Test_accy 53.64
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 17/40 => Loss 0.260, Train_accy 92.80, Test_accy 53.84
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 18/40 => Loss 0.270, Train_accy 92.60, Test_accy 52.82
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 19/40 => Loss 0.264, Train_accy 92.58, Test_accy 52.74
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 20/40 => Loss 0.256, Train_accy 92.76, Test_accy 52.12
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 21/40 => Loss 0.279, Train_accy 92.14, Test_accy 52.34
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 22/40 => Loss 0.280, Train_accy 92.24, Test_accy 50.92
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 23/40 => Loss 0.273, Train_accy 92.22, Test_accy 52.08
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 24/40 => Loss 0.252, Train_accy 92.64, Test_accy 51.74
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 25/40 => Loss 0.255, Train_accy 93.18, Test_accy 51.40
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 26/40 => Loss 0.261, Train_accy 92.94, Test_accy 51.12
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 27/40 => Loss 0.253, Train_accy 93.04, Test_accy 51.40
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 28/40 => Loss 0.268, Train_accy 92.56, Test_accy 51.52
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 29/40 => Loss 0.253, Train_accy 92.84, Test_accy 51.48
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 30/40 => Loss 0.271, Train_accy 92.12, Test_accy 50.96
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 31/40 => Loss 0.243, Train_accy 93.12, Test_accy 51.42
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 32/40 => Loss 0.249, Train_accy 92.88, Test_accy 51.34
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 33/40 => Loss 0.249, Train_accy 93.16, Test_accy 51.42
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 34/40 => Loss 0.263, Train_accy 92.96, Test_accy 51.58
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 35/40 => Loss 0.252, Train_accy 92.84, Test_accy 51.34
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 36/40 => Loss 0.253, Train_accy 92.72, Test_accy 51.50
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 37/40 => Loss 0.258, Train_accy 93.12, Test_accy 51.56
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 38/40 => Loss 0.239, Train_accy 93.08, Test_accy 51.56
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 4, Epoch 39/40 => Loss 0.258, Train_accy 92.60, Test_accy 51.52
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 4, Epoch 40/40 => Loss 0.255, Train_accy 92.88, Test_accy 51.50
Cache size: 79
Task 4, Epoch 40/40 => Loss 0.255, Train_accy 92.88, Test_accy 51.50
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.19it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 40 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 41 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 42 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 43 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 44 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 45 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 46 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 47 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.25it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 48 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 49 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  2%|▏         | 1/50 [00:00<00:09,  5.24it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.54it/s]
best_loss: 0.09427777796983719
Computing class relations...
Old means shape: (40, 768)
New means shape: (10, 768)
Class relations: [46 47 46 46 46 47 46 46 46 43 46 47 46 47 46 46 46 43 46 46 42 47 47 47
 42 43 43 45 46 47 46 40 40 41 43 46 48 45 48 49]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 25000
Feature dataset dimension: 768
Label dataset size: 25000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 50])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 391/391 [00:14<00:00, 27.36batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 4 Elapsed time:  3851.3957014083862
Average Accuracy (CNN): 92.666
Learning on 50-60
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([50, 768])
New weight (Cosine FC) torch.Size([60, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=60, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([50, 5000])
New weight (AC model) torch.Size([60, 5000])
91,174,369 total parameters.
1,235,713 training parameters.
Progressive training for task 5
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 1/40 => Loss 0.688, Train_accy 84.38, Test_accy 59.28
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 2/40 => Loss 0.377, Train_accy 91.20, Test_accy 57.90
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 3/40 => Loss 0.351, Train_accy 91.66, Test_accy 54.43
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 4/40 => Loss 0.306, Train_accy 92.28, Test_accy 55.07
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 5/40 => Loss 0.315, Train_accy 92.60, Test_accy 53.17
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 6/40 => Loss 0.280, Train_accy 93.02, Test_accy 52.73
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 7/40 => Loss 0.289, Train_accy 93.16, Test_accy 52.53
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 8/40 => Loss 0.270, Train_accy 93.22, Test_accy 50.93
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 9/40 => Loss 0.280, Train_accy 93.08, Test_accy 51.32
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 10/40 => Loss 0.250, Train_accy 93.72, Test_accy 53.47
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 11/40 => Loss 0.254, Train_accy 93.40, Test_accy 50.40
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 12/40 => Loss 0.265, Train_accy 93.32, Test_accy 50.10
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 13/40 => Loss 0.256, Train_accy 93.18, Test_accy 49.23
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 14/40 => Loss 0.239, Train_accy 93.56, Test_accy 50.05
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 15/40 => Loss 0.253, Train_accy 93.46, Test_accy 50.00
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 16/40 => Loss 0.220, Train_accy 94.18, Test_accy 50.65
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 17/40 => Loss 0.239, Train_accy 93.80, Test_accy 49.52
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 18/40 => Loss 0.232, Train_accy 93.60, Test_accy 47.70
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 19/40 => Loss 0.233, Train_accy 93.76, Test_accy 47.78
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 20/40 => Loss 0.243, Train_accy 93.66, Test_accy 47.73
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 21/40 => Loss 0.219, Train_accy 94.30, Test_accy 48.62
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 22/40 => Loss 0.234, Train_accy 93.42, Test_accy 47.50
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 23/40 => Loss 0.209, Train_accy 94.34, Test_accy 48.45
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 24/40 => Loss 0.220, Train_accy 93.82, Test_accy 47.88
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 25/40 => Loss 0.220, Train_accy 94.28, Test_accy 46.95
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 26/40 => Loss 0.216, Train_accy 94.28, Test_accy 48.05
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 27/40 => Loss 0.225, Train_accy 93.82, Test_accy 47.32
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 28/40 => Loss 0.208, Train_accy 94.54, Test_accy 47.52
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 29/40 => Loss 0.206, Train_accy 94.62, Test_accy 48.00
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 30/40 => Loss 0.206, Train_accy 94.38, Test_accy 46.83
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 31/40 => Loss 0.201, Train_accy 94.62, Test_accy 47.02
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 32/40 => Loss 0.213, Train_accy 94.46, Test_accy 46.05
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 33/40 => Loss 0.206, Train_accy 94.54, Test_accy 46.33
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 34/40 => Loss 0.204, Train_accy 94.72, Test_accy 46.50
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 35/40 => Loss 0.201, Train_accy 94.70, Test_accy 46.80
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 36/40 => Loss 0.191, Train_accy 94.94, Test_accy 46.78
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 37/40 => Loss 0.219, Train_accy 94.08, Test_accy 46.52
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 38/40 => Loss 0.198, Train_accy 94.88, Test_accy 46.52
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 5, Epoch 39/40 => Loss 0.212, Train_accy 94.50, Test_accy 46.58
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 5, Epoch 40/40 => Loss 0.213, Train_accy 94.08, Test_accy 46.58
Cache size: 79
Task 5, Epoch 40/40 => Loss 0.213, Train_accy 94.08, Test_accy 46.58
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.20it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 50 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 51 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 52 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 53 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 54 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 55 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 56 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 57 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 58 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 59 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  2%|▏         | 1/50 [00:00<00:09,  5.13it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:08<00:00,  5.56it/s]
best_loss: 0.08225036334991455
Computing class relations...
Old means shape: (50, 768)
New means shape: (10, 768)
Class relations: [57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57 57
 57 57 57 57 57 57 57 51 57 57 57 59 57 57 57 57 50 57 57 57 54 55 59 57
 56 50]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 30000
Feature dataset dimension: 768
Label dataset size: 30000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 60])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 469/469 [00:17<00:00, 27.28batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 5 Elapsed time:  4111.630991458893
Average Accuracy (CNN): 92.11
Learning on 60-70
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([60, 768])
New weight (Cosine FC) torch.Size([70, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=70, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([60, 5000])
New weight (AC model) torch.Size([70, 5000])
91,232,049 total parameters.
1,243,393 training parameters.
Progressive training for task 6
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 1/40 => Loss 0.931, Train_accy 78.54, Test_accy 53.24
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 2/40 => Loss 0.582, Train_accy 86.04, Test_accy 51.63
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 3/40 => Loss 0.536, Train_accy 86.84, Test_accy 48.41
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 4/40 => Loss 0.505, Train_accy 87.84, Test_accy 48.17
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 5/40 => Loss 0.487, Train_accy 87.54, Test_accy 47.54
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 6/40 => Loss 0.464, Train_accy 88.10, Test_accy 46.93
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 7/40 => Loss 0.458, Train_accy 88.12, Test_accy 46.01
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 8/40 => Loss 0.456, Train_accy 88.02, Test_accy 43.87
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 9/40 => Loss 0.451, Train_accy 87.72, Test_accy 45.09
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 10/40 => Loss 0.437, Train_accy 88.42, Test_accy 43.70
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 11/40 => Loss 0.409, Train_accy 89.10, Test_accy 43.94
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 12/40 => Loss 0.432, Train_accy 88.42, Test_accy 43.59
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 13/40 => Loss 0.426, Train_accy 88.86, Test_accy 41.21
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 14/40 => Loss 0.421, Train_accy 88.32, Test_accy 41.91
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 15/40 => Loss 0.395, Train_accy 89.54, Test_accy 41.86
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 16/40 => Loss 0.403, Train_accy 89.12, Test_accy 42.97
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 17/40 => Loss 0.385, Train_accy 89.64, Test_accy 40.61
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 18/40 => Loss 0.393, Train_accy 89.52, Test_accy 40.69
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 19/40 => Loss 0.394, Train_accy 89.84, Test_accy 41.31
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 20/40 => Loss 0.388, Train_accy 89.08, Test_accy 40.27
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 21/40 => Loss 0.396, Train_accy 88.98, Test_accy 39.14
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 22/40 => Loss 0.398, Train_accy 89.18, Test_accy 39.66
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 23/40 => Loss 0.377, Train_accy 89.60, Test_accy 39.09
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 24/40 => Loss 0.372, Train_accy 89.90, Test_accy 40.59
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 25/40 => Loss 0.343, Train_accy 90.12, Test_accy 40.01
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 26/40 => Loss 0.364, Train_accy 89.88, Test_accy 39.24
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 27/40 => Loss 0.368, Train_accy 89.46, Test_accy 39.06
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 28/40 => Loss 0.377, Train_accy 89.60, Test_accy 39.41
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 29/40 => Loss 0.343, Train_accy 90.72, Test_accy 39.84
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 30/40 => Loss 0.372, Train_accy 89.66, Test_accy 39.69
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 31/40 => Loss 0.346, Train_accy 90.50, Test_accy 39.29
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 32/40 => Loss 0.345, Train_accy 90.54, Test_accy 39.39
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 33/40 => Loss 0.362, Train_accy 89.82, Test_accy 39.14
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 34/40 => Loss 0.350, Train_accy 90.24, Test_accy 38.91
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 35/40 => Loss 0.344, Train_accy 90.66, Test_accy 38.63
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 36/40 => Loss 0.367, Train_accy 89.74, Test_accy 38.50
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 37/40 => Loss 0.361, Train_accy 90.16, Test_accy 38.54
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 6, Epoch 38/40 => Loss 0.343, Train_accy 90.86, Test_accy 38.66
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 39/40 => Loss 0.353, Train_accy 89.94, Test_accy 38.59
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 6, Epoch 40/40 => Loss 0.335, Train_accy 90.38, Test_accy 38.63
Cache size: 79
Task 6, Epoch 40/40 => Loss 0.335, Train_accy 90.38, Test_accy 38.63
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.21it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 60 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 61 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 62 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.25it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 63 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 64 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 65 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 66 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 67 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 68 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 69 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  2%|▏         | 1/50 [00:00<00:09,  5.03it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.55it/s]
best_loss: 0.11310800927877426
Computing class relations...
Old means shape: (60, 768)
New means shape: (10, 768)
Class relations: [62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62 62
 67 62 62 62 62 62 62 67 67 67 62 67 67 62 67 67 65 62 62 62 65 67 67 62
 67 63 63 65 63 69 65 68 68 62 61 69]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 35000
Feature dataset dimension: 768
Label dataset size: 35000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 70])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 547/547 [00:20<00:00, 27.23batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 6 Elapsed time:  4372.7201681137085
Average Accuracy (CNN): 91.55142857142857
Learning on 70-80
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([70, 768])
New weight (Cosine FC) torch.Size([80, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=80, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([70, 5000])
New weight (AC model) torch.Size([80, 5000])
91,289,729 total parameters.
1,251,073 training parameters.
Progressive training for task 7
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 1/40 => Loss 0.851, Train_accy 80.30, Test_accy 54.76
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 2/40 => Loss 0.489, Train_accy 88.70, Test_accy 53.01
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 3/40 => Loss 0.461, Train_accy 88.56, Test_accy 51.32
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 4/40 => Loss 0.411, Train_accy 89.72, Test_accy 51.12
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 5/40 => Loss 0.400, Train_accy 90.56, Test_accy 49.29
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 6/40 => Loss 0.369, Train_accy 90.90, Test_accy 50.25
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 7/40 => Loss 0.381, Train_accy 90.36, Test_accy 49.72
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 8/40 => Loss 0.350, Train_accy 91.16, Test_accy 48.65
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 9/40 => Loss 0.357, Train_accy 90.96, Test_accy 47.10
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 10/40 => Loss 0.329, Train_accy 91.90, Test_accy 48.62
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 11/40 => Loss 0.345, Train_accy 91.16, Test_accy 46.21
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 12/40 => Loss 0.319, Train_accy 91.86, Test_accy 48.30
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 13/40 => Loss 0.304, Train_accy 91.98, Test_accy 48.28
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 14/40 => Loss 0.308, Train_accy 91.74, Test_accy 47.04
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 15/40 => Loss 0.312, Train_accy 91.72, Test_accy 47.01
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 16/40 => Loss 0.301, Train_accy 92.32, Test_accy 47.32
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 17/40 => Loss 0.305, Train_accy 92.14, Test_accy 47.05
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 18/40 => Loss 0.306, Train_accy 91.90, Test_accy 46.38
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 19/40 => Loss 0.306, Train_accy 92.02, Test_accy 46.82
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 20/40 => Loss 0.275, Train_accy 92.68, Test_accy 46.79
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 21/40 => Loss 0.288, Train_accy 92.46, Test_accy 45.71
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 22/40 => Loss 0.286, Train_accy 92.44, Test_accy 45.34
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 23/40 => Loss 0.284, Train_accy 92.72, Test_accy 46.18
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 24/40 => Loss 0.294, Train_accy 92.00, Test_accy 45.38
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 25/40 => Loss 0.292, Train_accy 92.06, Test_accy 45.81
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 26/40 => Loss 0.280, Train_accy 92.32, Test_accy 45.46
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 27/40 => Loss 0.288, Train_accy 91.96, Test_accy 44.95
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 28/40 => Loss 0.303, Train_accy 91.64, Test_accy 44.78
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 29/40 => Loss 0.288, Train_accy 92.06, Test_accy 44.26
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 30/40 => Loss 0.279, Train_accy 92.44, Test_accy 44.24
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 31/40 => Loss 0.278, Train_accy 92.30, Test_accy 44.06
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 32/40 => Loss 0.292, Train_accy 91.92, Test_accy 43.98
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 33/40 => Loss 0.264, Train_accy 92.90, Test_accy 44.19
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 34/40 => Loss 0.273, Train_accy 92.32, Test_accy 44.74
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 35/40 => Loss 0.277, Train_accy 92.36, Test_accy 44.49
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 7, Epoch 36/40 => Loss 0.266, Train_accy 92.72, Test_accy 44.31
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 37/40 => Loss 0.277, Train_accy 92.52, Test_accy 44.54
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 38/40 => Loss 0.284, Train_accy 92.14, Test_accy 44.39
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 39/40 => Loss 0.277, Train_accy 92.84, Test_accy 44.44
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 7, Epoch 40/40 => Loss 0.278, Train_accy 92.70, Test_accy 44.42
Cache size: 79
Task 7, Epoch 40/40 => Loss 0.278, Train_accy 92.70, Test_accy 44.42
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 70 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 71 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 72 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 73 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 74 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.25it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 75 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 76 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 77 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 78 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.15it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 79 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  2%|▏         | 1/50 [00:00<00:09,  5.03it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.48it/s]
best_loss: 0.11912328147888183
Computing class relations...
Old means shape: (70, 768)
New means shape: (10, 768)
Class relations: [71 71 71 71 70 71 71 71 71 70 70 70 70 70 70 70 70 70 70 71 70 70 70 70
 70 70 70 70 70 70 70 77 70 70 70 71 71 70 71 70 77 70 70 70 77 77 71 76
 71 78 78 77 78 71 77 77 78 76 70 71 73 79 76 73 71 74 77 71 75 71]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 40000
Feature dataset dimension: 768
Label dataset size: 40000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 80])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 625/625 [00:22<00:00, 27.22batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 7 Elapsed time:  4633.355473279953
Average Accuracy (CNN): 90.98375
Learning on 80-90
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([80, 768])
New weight (Cosine FC) torch.Size([90, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=90, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([80, 5000])
New weight (AC model) torch.Size([90, 5000])
91,347,409 total parameters.
1,258,753 training parameters.
Progressive training for task 8
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 1/40 => Loss 0.608, Train_accy 86.90, Test_accy 51.33
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 2/40 => Loss 0.324, Train_accy 92.00, Test_accy 49.29
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 3/40 => Loss 0.276, Train_accy 92.54, Test_accy 48.34
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 4/40 => Loss 0.265, Train_accy 92.78, Test_accy 47.11
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 5/40 => Loss 0.263, Train_accy 93.36, Test_accy 47.68
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 6/40 => Loss 0.266, Train_accy 92.64, Test_accy 47.07
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 7/40 => Loss 0.255, Train_accy 93.20, Test_accy 46.83
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 8/40 => Loss 0.245, Train_accy 93.46, Test_accy 45.26
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 9/40 => Loss 0.255, Train_accy 93.16, Test_accy 44.51
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 10/40 => Loss 0.229, Train_accy 93.74, Test_accy 44.39
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 11/40 => Loss 0.237, Train_accy 93.44, Test_accy 44.66
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 12/40 => Loss 0.244, Train_accy 93.32, Test_accy 44.39
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 13/40 => Loss 0.261, Train_accy 93.06, Test_accy 42.48
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 14/40 => Loss 0.226, Train_accy 94.20, Test_accy 43.77
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 15/40 => Loss 0.236, Train_accy 93.48, Test_accy 43.38
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 16/40 => Loss 0.223, Train_accy 93.72, Test_accy 43.07
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 17/40 => Loss 0.222, Train_accy 93.98, Test_accy 42.90
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 18/40 => Loss 0.204, Train_accy 94.60, Test_accy 43.14
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 19/40 => Loss 0.221, Train_accy 93.78, Test_accy 42.21
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 20/40 => Loss 0.222, Train_accy 93.40, Test_accy 42.19
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 21/40 => Loss 0.200, Train_accy 94.32, Test_accy 43.73
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 22/40 => Loss 0.220, Train_accy 93.72, Test_accy 42.94
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 23/40 => Loss 0.209, Train_accy 93.96, Test_accy 42.41
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 24/40 => Loss 0.213, Train_accy 94.34, Test_accy 41.97
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 25/40 => Loss 0.210, Train_accy 94.22, Test_accy 42.42
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 26/40 => Loss 0.212, Train_accy 94.22, Test_accy 42.40
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 27/40 => Loss 0.197, Train_accy 94.38, Test_accy 42.50
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 28/40 => Loss 0.207, Train_accy 94.18, Test_accy 42.03
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 29/40 => Loss 0.227, Train_accy 93.96, Test_accy 41.80
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 30/40 => Loss 0.217, Train_accy 93.90, Test_accy 41.19
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 31/40 => Loss 0.202, Train_accy 94.32, Test_accy 41.82
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 32/40 => Loss 0.212, Train_accy 94.06, Test_accy 41.57
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 33/40 => Loss 0.218, Train_accy 93.92, Test_accy 41.57
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 34/40 => Loss 0.212, Train_accy 94.42, Test_accy 41.47
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 35/40 => Loss 0.207, Train_accy 93.96, Test_accy 41.52
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 36/40 => Loss 0.204, Train_accy 94.20, Test_accy 41.41
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 8, Epoch 37/40 => Loss 0.206, Train_accy 94.36, Test_accy 41.33
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 38/40 => Loss 0.197, Train_accy 94.70, Test_accy 41.32
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 39/40 => Loss 0.209, Train_accy 94.12, Test_accy 41.32
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 8, Epoch 40/40 => Loss 0.208, Train_accy 94.12, Test_accy 41.33
Cache size: 79
Task 8, Epoch 40/40 => Loss 0.208, Train_accy 94.12, Test_accy 41.33
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 80 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.25it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 81 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 82 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 83 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 84 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 85 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 86 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 87 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 88 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.19it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 89 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  2%|▏         | 1/50 [00:00<00:09,  5.23it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:08<00:00,  5.57it/s]
best_loss: 0.09248087340593338
Computing class relations...
Old means shape: (80, 768)
New means shape: (10, 768)
Class relations: [80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80
 80 80 80 80 80 80 80 84 80 80 80 80 84 80 80 80 84 80 80 80 84 84 80 80
 80 84 84 84 84 84 84 84 84 80 80 84 83 80 80 84 80 84 84 80 84 84 80 87
 81 86 87 83 80 84 86 80]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 45000
Feature dataset dimension: 768
Label dataset size: 45000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 90])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 704/704 [00:25<00:00, 27.26batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 8 Elapsed time:  4894.286454677582
Average Accuracy (CNN): 90.41666666666667
Learning on 90-100
Train dataset size: 5000
Use Cosine model as classifier head.
Cosine model architecture: CosineLinear2()
Old weight (Cosine FC) torch.Size([90, 768])
New weight (Cosine FC) torch.Size([100, 768])
Use AC model as classifier head.
AC model architecture: AC_Linear(
  (fc): Sequential(
    (0): Linear(in_features=768, out_features=5000, bias=False)
    (1): ReLU()
    (2): Linear(in_features=5000, out_features=100, bias=False)
  )
)
Hidden weight (AC model) torch.Size([5000, 768])
Old weight (AC model) torch.Size([90, 5000])
New weight (AC model) torch.Size([100, 5000])
91,405,089 total parameters.
1,266,433 training parameters.
Progressive training for task 9
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 1/40 => Loss 0.699, Train_accy 84.20, Test_accy 52.54
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 2/40 => Loss 0.360, Train_accy 91.06, Test_accy 49.09
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 3/40 => Loss 0.323, Train_accy 91.92, Test_accy 48.24
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 4/40 => Loss 0.309, Train_accy 92.44, Test_accy 47.49
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 5/40 => Loss 0.293, Train_accy 92.48, Test_accy 46.49
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 6/40 => Loss 0.287, Train_accy 92.62, Test_accy 47.12
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 9, Epoch 7/40 => Loss 0.264, Train_accy 93.22, Test_accy 44.68
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 8/40 => Loss 0.268, Train_accy 93.02, Test_accy 44.29
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 9, Epoch 9/40 => Loss 0.282, Train_accy 92.36, Test_accy 42.03
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 10/40 => Loss 0.277, Train_accy 92.88, Test_accy 43.09
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 11/40 => Loss 0.258, Train_accy 93.16, Test_accy 42.64
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 12/40 => Loss 0.244, Train_accy 93.56, Test_accy 43.45
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 13/40 => Loss 0.259, Train_accy 93.30, Test_accy 42.83
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 9, Epoch 14/40 => Loss 0.246, Train_accy 93.36, Test_accy 41.49
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 9, Epoch 15/40 => Loss 0.237, Train_accy 93.40, Test_accy 42.05
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 16/40 => Loss 0.231, Train_accy 93.52, Test_accy 42.14
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 17/40 => Loss 0.242, Train_accy 93.54, Test_accy 40.27
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 18/40 => Loss 0.214, Train_accy 94.22, Test_accy 41.50
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 19/40 => Loss 0.243, Train_accy 93.70, Test_accy 39.20
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 20/40 => Loss 0.230, Train_accy 93.84, Test_accy 40.07
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 9, Epoch 21/40 => Loss 0.241, Train_accy 93.32, Test_accy 39.66
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 9, Epoch 22/40 => Loss 0.245, Train_accy 93.38, Test_accy 39.33
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 9, Epoch 23/40 => Loss 0.245, Train_accy 93.30, Test_accy 38.87
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 24/40 => Loss 0.228, Train_accy 93.72, Test_accy 39.45
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 9, Epoch 25/40 => Loss 0.227, Train_accy 93.54, Test_accy 38.75
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 9, Epoch 26/40 => Loss 0.227, Train_accy 93.52, Test_accy 38.55
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 27/40 => Loss 0.210, Train_accy 94.50, Test_accy 39.22
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 9, Epoch 28/40 => Loss 0.208, Train_accy 94.32, Test_accy 39.11
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 29/40 => Loss 0.238, Train_accy 93.34, Test_accy 39.15
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 30/40 => Loss 0.219, Train_accy 94.00, Test_accy 38.37
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 9, Epoch 31/40 => Loss 0.207, Train_accy 94.40, Test_accy 38.39
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 32/40 => Loss 0.213, Train_accy 94.28, Test_accy 38.55
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 33/40 => Loss 0.216, Train_accy 94.04, Test_accy 37.99
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 34/40 => Loss 0.221, Train_accy 93.66, Test_accy 38.15
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 35/40 => Loss 0.215, Train_accy 94.42, Test_accy 38.20
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 36/40 => Loss 0.209, Train_accy 93.98, Test_accy 38.22
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 9, Epoch 37/40 => Loss 0.207, Train_accy 94.00, Test_accy 37.98
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 38/40 => Loss 0.207, Train_accy 94.52, Test_accy 38.03
100%|██████████| 79/79 [01:01<00:00,  1.29it/s]
Task 9, Epoch 39/40 => Loss 0.196, Train_accy 94.58, Test_accy 38.06
100%|██████████| 79/79 [01:00<00:00,  1.30it/s]
Task 9, Epoch 40/40 => Loss 0.211, Train_accy 94.12, Test_accy 38.08
Cache size: 79
Task 9, Epoch 40/40 => Loss 0.211, Train_accy 94.12, Test_accy 38.08
Computing class means and covariance matrices...
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 90 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 91 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 92 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 93 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 94 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 95 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 96 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 97 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 98 covariance matrix shape: (768, 768)
Extracting prototypes...
100%|██████████| 8/8 [00:03<00:00,  2.19it/s]
Extracted vectors shape: (500, 768) float32
Extracted targets shape: (500,) int64
Class 99 covariance matrix shape: (768, 768)
Generating pseudo-features for old classes from relations...
Calibrating prototype model (Prototype correction - Knowledge Rumination)...
  2%|▏         | 1/50 [00:00<00:09,  5.12it/s]
开始 修正 prototype
100%|██████████| 50/50 [00:09<00:00,  5.39it/s]
best_loss: 0.0700051703453064
Computing class relations...
Old means shape: (90, 768)
New means shape: (10, 768)
Class relations: [97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97
 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 97 96 97 97 97 96 97 97 97
 97 97 93 96 97 96 96 96 96 97 97 96 92 97 97 93 96 97 96 97 96 96 97 94
 93 93 92 93 97 96 93 97 97 90 97 90 99 99 93 98 99 96]
Building feature dataset...
Extract prototypes for known classes...
Total feature dataset size: 50000
Feature dataset dimension: 768
Label dataset size: 50000
Label dataset classes: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95
 96 97 98 99]
Incremental class alignment (Knowledge Memorization)...
Knowledge Memorization completed.
Updated weight matrix W shape: torch.Size([5000, 100])
Updated correlation matrix R shape: torch.Size([5000, 5000])
numpy inverse
Calibrating classifier weights (Knowledge Rumination - Selective reinforcement of old task knowledge)...
Alignment: 100%|██████████| 782/782 [00:28<00:00, 27.26batch/s]
numpy inverse
Finish one task 
[TIME TIME TIME] Task= 9 Elapsed time:  5155.118850946426
Average Accuracy (CNN): 89.924